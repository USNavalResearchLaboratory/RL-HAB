{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from fluids import ATMOSPHERE_1976\n",
    "\n",
    "import xarray as xr \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib import cm\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import re\n",
    "#matplotlib notebook\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(base_dir, month, day, write=False):\n",
    "\n",
    "    \"\"\"With a folder containing multiple station's worth of Radiosonde data, create a concatenated CSV of all radiosonde data from the \n",
    "    given day of the given month.\\\\\n",
    "    \n",
    "    base_dir: Directory containing all station data folders.\\\\\n",
    "    month: integer from 1-12\\\\\n",
    "    day: integer from 1-x, based on which month you select\"\"\"\n",
    "\n",
    "    print('AGGREGATION... ')\n",
    "\n",
    "    csv_list = []\n",
    "\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder+'/2023/') #Can modify this for the year\n",
    "\n",
    "        folder_path = os.path.join(folder_path, str(month)) #create a path to the folder for the month you want data from\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "                #Finds every csv in the selected month folder\n",
    "                csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "                if csv_files:\n",
    "                    # Read the X-th (x is the day you choose) CSV file in the selected folder\n",
    "                    first_csv_path = os.path.join(folder_path, csv_files[day])\n",
    "                    \n",
    "                    # Append the path to the list\n",
    "                    csv_list.append(first_csv_path)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through the csv_list and read each CSV file\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        #create a new column for the date and time of the reading\n",
    "        datetime = csv_path.split('/')[-1].split('.')[0].split('-')[1:]\n",
    "        timestep = '-'.join(datetime)\n",
    "        df['TIMESTEP'] = timestep\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all the DataFrames\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True).drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file if write == True\n",
    "    if write == True: \n",
    "        output_csv_path = '/path/to/save/concatenated.csv'\n",
    "        concatenated_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print('AGGREGATION : DONE.')\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data2(base_dir, year, month, day, hour, write=False):\n",
    "\n",
    "    \"\"\"With a folder containing multiple station's worth of Radiosonde data, create a concatenated CSV of all radiosonde data from the \n",
    "    given day of the given month.\\\\\n",
    "    \n",
    "    base_dir: Directory containing all station data folders.\\\\\n",
    "    month: integer from 1-12\\\\\n",
    "    day: integer from 1-x, based on which month you select\"\"\"\n",
    "\n",
    "    print('AGGREGATION... ')\n",
    "\n",
    "    csv_list = []\n",
    "\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder+'/' + str(year) + '/') \n",
    "\n",
    "        folder_path = os.path.join(folder_path, str(month)) \n",
    "\n",
    "        #Iterate through every station for a particular month and year (not necessarily alphabetical order)\n",
    "        if os.path.isdir(folder_path):\n",
    "                #Check if the station has data for that month\n",
    "                csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "                if csv_files:\n",
    "                    #Parse out the station info for the CSV\n",
    "                    string_parse_pattern = r'^[^-]+-[^-]+-'\n",
    "                    # Search the pattern in the string\n",
    "                    station_info = re.match(string_parse_pattern, csv_files[0])\n",
    "                    station_info = station_info.group(0)\n",
    "\n",
    "                    #Create the correct csv string name for year, month, day, and time for that particular station\n",
    "                    selected_csv  = os.path.join(folder_path, station_info + str(month) + '-' + str(day) + '-' + str(hour) + '.csv')\n",
    "\n",
    "                    #If the CSV exists, add it to the array for parsing, otherwise skip.\n",
    "                    if os.path.isfile(selected_csv):\n",
    "                        # Append the path to the list\n",
    "                        csv_list.append(selected_csv)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through the csv_list and read each CSV file\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        #create a new column for the date and time of the reading\n",
    "        datetime = csv_path.split('/')[-1].split('.')[0].split('-')[1:]\n",
    "        timestep = '-'.join(datetime)\n",
    "        df['TIMESTEP'] = timestep\n",
    "\n",
    "\n",
    "        #THIS IS ALL NEW PREPROCESSING STUFF DURRING AGGREGATION\n",
    "\n",
    "        # Drop rows where 'height' is NaN\n",
    "        df = df.dropna(subset=['height'])\n",
    "        # Create a new column for the rounded heights to 250m\n",
    "        df['rounded_height'] = df['height'].apply(lambda x: round(x / 250) * 250)\n",
    "\n",
    "        #df['rounded_pressure'] = df['height'].apply(lambda x: round(x / 250) * 250)\n",
    "        \n",
    "        # Calculate the absolute difference between original and rounded heights\n",
    "        df['abs_diff'] = abs(df['height'] - df['rounded_height'])\n",
    "\n",
    "        # Sort by rounded height and absolute difference\n",
    "        df = df.sort_values(by=['rounded_height', 'abs_diff'])\n",
    "\n",
    "        # Drop duplicates, keeping the row with the smallest difference\n",
    "        df = df.drop_duplicates(subset='rounded_height', keep='first')\n",
    "\n",
    "        \n",
    "\n",
    "        # Create a new DataFrame with the desired height intervals\n",
    "        intervals = np.arange(0, 32001, 250)  # Heights from 0 to 30000 at 250m intervals\n",
    "        df_intervals = pd.DataFrame({'rounded_height': intervals})\n",
    "\n",
    "        # Merge the rounded DataFrame with the new intervals\n",
    "        df_resampled = df_resampled = pd.merge(df_intervals, df, on='rounded_height', how='left')\n",
    "\n",
    "        df_resampled['height'] = df_resampled['rounded_height']\n",
    "        df_resampled = df_resampled.drop(columns=['rounded_height', 'abs_diff'])\n",
    "\n",
    "        #print(df_resampled)\n",
    "\n",
    "        df_list.append(df_resampled)\n",
    "\n",
    "    # Concatenate all the DataFrames\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True).drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file if write == True\n",
    "    if write == True: \n",
    "        output_csv_path = '/path/to/save/concatenated.csv'\n",
    "        concatenated_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "    concatenated_df = concatenated_df[['pressure','height', 'direction', 'speed', 'u_wind', 'v_wind', 'time', 'latitude', 'longitude', 'TIMESTEP']]\n",
    "\n",
    "    #print(concatenated_df)\n",
    "    #print(concatenated_df.height.max())\n",
    "    # Drop rows where where there are any nan values\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    \n",
    "    print('AGGREGATION : DONE.')\n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_level_original(df, altitude):\n",
    "    \n",
    "    df= df[(df['height'] >=  altitude-125)& (df['height'] <= altitude+125)]\n",
    "\n",
    "    x = df['longitude'].values\n",
    "    y = df['latitude'].values\n",
    "    z = df['height'].values\n",
    "    u = df['u_wind'].values\n",
    "    v = df['v_wind'].values\n",
    "    w = np.zeros_like(u)\n",
    "    speed= df['speed']#.astype('int')\n",
    "    \n",
    "    norm = plt.Normalize(speed.min(), speed.max())\n",
    "    colors = cm.hsv(norm(speed))\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    mappable = cm.ScalarMappable(cmap=cm.hsv, norm=norm)\n",
    "    mappable.set_array(speed)\n",
    "    cbar = plt.colorbar(mappable, ax=ax, fraction = 0.03, pad=0.1)\n",
    "    cbar.set_label('Wind Speed')\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        ax.quiver(x[i], y[i], z[i], u[i], v[i], w[i], colors=colors[i], length=2, arrow_length_ratio=0.5, normalize=True)\n",
    "    \n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_zlabel('Pressure Level')\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_stations(df, lat_min, lat_max, lon_min, lon_max, step, alt_step=250):\n",
    "\n",
    "    \"\"\"Use: Create a (optionally) high resolution 3-D grid of radiosonde data within a specific longitude-latitude and altitude range, \n",
    "    provided you have prior data to synthesize from.\\\\\n",
    "    \n",
    "    df: Dataframe produced by aggregate_data function, based on existing radiosonde data.\\\\\n",
    "    lat_min: minimum latitude of region determined by df.\\\\\n",
    "    lat_max: maximum latitude of region determined by df.\\\\\n",
    "    lon_min: minimum longitude of region determined by df.\\\\\n",
    "    lon_max: maximum longitude of region determined by df.\\\\\n",
    "    step: resolution in latitude-longitude for which to generate data. (Ex. step = 1 means data will be generated each 1 lat-lon degrees).\\\\\n",
    "    alt_step: Vertical resolution in meters for which to generate data. (Ex. alt_step = 250 means data will be generated every 250 meters.)\n",
    "    \"\"\"\n",
    "\n",
    "    print('SYNTHESIZING...')\n",
    "\n",
    "    #Creates a range of lats/lons used for coordiantes when making data.\n",
    "    latitudes = np.arange(lat_min, lat_max + step, step) \n",
    "    longitudes = np.arange(lon_min, lon_max + step, step)\n",
    "    \n",
    "    # Define altitude range based on the min and max altitudes in the original data\n",
    "    alt_min, alt_max = df['height'].min(), df['height'].max()\n",
    "    altitudes = np.arange(alt_min, alt_max + alt_step, alt_step)\n",
    "    \n",
    "    synthetic_data = []\n",
    "\n",
    "    station_number = 0 \n",
    "\n",
    "    for lat in latitudes:\n",
    "        for lon in longitudes:\n",
    "            station_number += 1\n",
    "            # Interpolate data for each altitude level\n",
    "            for altitude in altitudes:\n",
    "                #Collect data into layers based on alt_step distance. \n",
    "                df_alt = df[(df['height'] >= altitude - alt_step/2) & (df['height'] < altitude + alt_step/2)]\n",
    "                if df_alt.empty:\n",
    "                    continue\n",
    "                \n",
    "                #Collect all features corresponding to selected lat/lon ranges. \n",
    "                points = df_alt[['latitude', 'longitude']].values\n",
    "                columns = ['pressure','height','temperature','direction','speed','u_wind','v_wind']\n",
    "                \n",
    "                interpolated_values = {}\n",
    "                for column in columns:\n",
    "                    values = df_alt[column].values\n",
    "                    #Utilize Nearest neighbor interpolation to fill empty spaces on 3d grid. \n",
    "                    interpolated_values[column] = griddata(points, values, (lat, lon), method='nearest')\n",
    "\n",
    "                synthetic_data.append({\n",
    "                    **interpolated_values,\n",
    "                    'station_number': station_number,\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'height': altitude\n",
    "                })\n",
    "    \n",
    "    synthetic_df = pd.DataFrame(synthetic_data)\n",
    "    print('SYNTHESIS : DONE.')\n",
    "    return synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_stations2(df, lat_min, lat_max, lon_min, lon_max, step, alt_step=250):\n",
    "\n",
    "    \"\"\"Use: Create a (optionally) high resolution 3-D grid of radiosonde data within a specific longitude-latitude and altitude range, \n",
    "    provided you have prior data to synthesize from.\\\\\n",
    "    \n",
    "    df: Dataframe produced by aggregate_data function, based on existing radiosonde data.\\\\\n",
    "    lat_min: minimum latitude of region determined by df.\\\\\n",
    "    lat_max: maximum latitude of region determined by df.\\\\\n",
    "    lon_min: minimum longitude of region determined by df.\\\\\n",
    "    lon_max: maximum longitude of region determined by df.\\\\\n",
    "    step: resolution in latitude-longitude for which to generate data. (Ex. step = 1 means data will be generated each 1 lat-lon degrees).\\\\\n",
    "    alt_step: Vertical resolution in meters for which to generate data. (Ex. alt_step = 250 means data will be generated every 250 meters.)\n",
    "    \"\"\"\n",
    "\n",
    "    print('SYNTHESIZING...')\n",
    "\n",
    "    #Creates a range of lats/lons used for coordiantes when making data.\n",
    "    latitudes = np.arange(lat_min, lat_max + step, step) \n",
    "    longitudes = np.arange(lon_min, lon_max + step, step)\n",
    "    lat_grid, lon_grid = np.meshgrid(latitudes, longitudes)\n",
    "\n",
    "    \n",
    "    # Define altitude range based on the min and max altitudes in the original data\n",
    "    alt_min, alt_max = df['height'].min(), df['height'].max()\n",
    "    altitudes = np.arange(alt_min, alt_max + alt_step, alt_step)\n",
    "    \n",
    "    synthetic_data = []\n",
    "\n",
    "    station_number = 0 \n",
    "\n",
    "\n",
    "    for altitude in altitudes:\n",
    "        #Collect data into layers based on alt_step distance. \n",
    "        df_alt = df[(df['height'] == altitude)]\n",
    "\n",
    "        #Check for missing data for interpolating at a particular altitude level\n",
    "        df_alt = df_alt.dropna(subset=['u_wind'])\n",
    "        \n",
    "        if df_alt.empty:\n",
    "            continue\n",
    "        \n",
    "        #Collect all features corresponding to selected lat/lon ranges. \n",
    "        points = df_alt[['latitude', 'longitude']].values\n",
    "        variables = {\n",
    "            'pressure': df_alt['pressure'].values,\n",
    "            'height': df_alt['height'].values,\n",
    "            #'temperature': df_alt['temperature'].values,\n",
    "            'direction': df_alt['direction'].values,\n",
    "            'speed': df_alt['speed'].values,\n",
    "            'u_wind': df_alt['u_wind'].values,\n",
    "            'v_wind': df_alt['v_wind'].values,\n",
    "        }\n",
    "\n",
    "\n",
    "        #Create initial shape\n",
    "        interpolated_values2 = griddata(points, variables['height'], (lat_grid, lon_grid), method='nearest')\n",
    "        df_interpolated = pd.DataFrame( interpolated_values2, index=longitudes, columns=latitudes)\n",
    "        df_interpolated = df_interpolated.reset_index()\n",
    "        df_interpolated = df_interpolated.melt(id_vars='index', var_name='latitude', value_name='height')\n",
    "        df_interpolated = df_interpolated.rename(columns={'index': 'longitude'})\n",
    "\n",
    "        # Perform interpolation\n",
    "        for var_name, values in variables.items():\n",
    "\n",
    "            #grid_values = griddata(points, values, (lat_grid, lon_grid), method='nearest')\n",
    "            interpolated_values = griddata(points, values, (lat_grid, lon_grid), method='nearest')\n",
    "\n",
    "            # Convert grid values to DataFrame\n",
    "            df_interpolated_var = pd.DataFrame( interpolated_values, index=longitudes, columns=latitudes)\n",
    "            df_interpolated_var = df_interpolated_var.reset_index()\n",
    "            df_interpolated_var = df_interpolated_var.melt(id_vars='index', var_name='latitude', value_name=var_name)\n",
    "            df_interpolated_var = df_interpolated_var.rename(columns={'index': 'longitude'})\n",
    "\n",
    "            df_interpolated[var_name] = df_interpolated_var[var_name]\n",
    "            \n",
    "            # Reorder columns\n",
    "            #df_interpolated = df_interpolated[['latitude', 'longitude', 'altitude']]\n",
    "            \n",
    "\n",
    "        synthetic_data.append(df_interpolated)\n",
    "\n",
    "    # Concatenate all the DataFrames\n",
    "    concatenated_df = pd.concat(synthetic_data, ignore_index=True)\n",
    "\n",
    "    # Sort by latitude, longitude, and altitude\n",
    "    concatenated_df = concatenated_df.sort_values(by=['latitude', 'longitude', 'height']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \n",
    "    print('SYNTHESIS : DONE.')\n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, lats, lons, sigma):\n",
    "\n",
    "    \"\"\"Use: Helper function that will smooth a variable on a single altitude layer.\\\\\n",
    "    x: list-like of variables needing to be smoothed.\\\\\n",
    "    lats: latitude dimension (Number of unique latitude values in data).\\\\\n",
    "    lons: longitude dimension (Number of unique longitude values in data).\\\\\n",
    "    sigma: Standard deviation of guassian kernel (Higher value = more homogenous result overall).\"\"\"\n",
    "\n",
    "    x = np.array(x).reshape((lats,lons)) #Reshape data into a matrix matching the geographic grid. \n",
    "    x1 = gaussian_filter(x, sigma=sigma) #apply smoothing across the matrix. \n",
    "    x1 = np.concatenate(x1, axis=0) #concatenate back to a numpy array for compatibility with later functions. \n",
    "\n",
    "    return x1\n",
    "\n",
    "def get_dims(df):\n",
    "    lats = df['latitude'].nunique()\n",
    "    lons = df['longitude'].nunique()\n",
    "    alts = df['height'].nunique()\n",
    "\n",
    "    return (lats, lons, alts)\n",
    "\n",
    "\n",
    "def glob_smooth(df, lats, lons, alts):\n",
    "\n",
    "    \"\"\"Use: With a synthetic dataframe produced by the create_synthetic_stations function, apply a gaussian smoothing \n",
    "    to each altitude layer and re-stack in order to produce a complete dtaframe of synthetic winds.\\\n",
    "    \n",
    "    df: Synthetic dataframe from create_synthetic_stations.\\\\\n",
    "    lats: Dimensions of latitude data. (Ex. Data has a latitude range of 70-110; lats = 40)\\\\\n",
    "    lons: Dimensions of longitude data. (Ex. Same logic as latitudes)\\\\\n",
    "    alts: Dimensions of altitude data. (Ex. Number of altitude levels in data) \"\"\"\n",
    "    \n",
    "    print(\"SMOOTHING...\")\n",
    "\n",
    "    speed = []\n",
    "    u = []\n",
    "    v = []\n",
    "    for i in range(0,alts): #For eahc altitude layer\n",
    "        for j in range(0,lats*lons): #For each station in the latitude-longitude grid \n",
    "            idx = alts * j + i #Each station index\n",
    "            speed.append(df['speed'][idx]) #Add data to a list for processing\n",
    "            u.append(df['u_wind'][idx])\n",
    "            v.append(df['v_wind'][idx])\n",
    "\n",
    "\n",
    "        #Apply helper function for smoothing all desired variables. \n",
    "        speed1 = smooth(speed, lats, lons, 3)\n",
    "        u1 = smooth(u, lats, lons, 12) # divide by res. so for .25 res is 12, for 1 degree is 3\n",
    "        v1 = smooth(v, lats, lons, 12)\n",
    "\n",
    "        for j in range(0, lats*lons): #Re-create a dataframe based on structure of origina ldata with synthetic values. \n",
    "            idx = alts * j + i\n",
    "            df.at[idx, 'speed'] = speed1[j]\n",
    "            df.at[idx, 'u_wind'] = u1[j]\n",
    "            df.at[idx, 'v_wind'] = v1[j]\n",
    "\n",
    "        speed.clear() #clear lists for use in next altitude layer. \n",
    "        u.clear()\n",
    "        v.clear()\n",
    "\n",
    "    \n",
    "    print('SMOOTHING : DONE.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer(df, type, height:int, skip = 1):\n",
    "    if height not in df['height'].unique():\n",
    "        raise ValueError(\"Invalid height value\")\n",
    "\n",
    "    if type == 'xy':\n",
    "        x = df[df['height'] == height ]['longitude'].values\n",
    "        y = df[df['height'] == height]['latitude'].values\n",
    "        u = df[df['height'] == height]['u_wind'].values\n",
    "        v = df[df['height'] == height]['v_wind'].values\n",
    "        w = np.zeros_like(u)\n",
    "        speed = df[df['height'] == height]['speed'].values\n",
    "\n",
    "        '''\n",
    "        u = u.astype(float)\n",
    "        v = v.astype(float)\n",
    "        print( u)\n",
    "        speed = np.sqrt(u**2 + v**2)\n",
    "        '''\n",
    "        #speed = speed/1.94\n",
    "        #speed = speed.astype('int')\n",
    "\n",
    "        u = np.asarray(u.tolist())\n",
    "        v = np.asarray(v.tolist())\n",
    "\n",
    "        # Calculate directions for color mapping\n",
    "        directions = np.arctan2(v, u)\n",
    "        speed = np.sqrt(v**2 + u**2)/1.94\n",
    "\n",
    "        # For Speed\n",
    "        #norm = plt.Normalize(np.min(speed), np.max(speed))\n",
    "        #colors = cm.hsv(norm(speed))\n",
    "\n",
    "        # For Direction\n",
    "        norm = plt.Normalize(-np.pi, np.pi)\n",
    "        colors = cm.hsv(norm(directions)) #for Directions\n",
    "        res = 1\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "\n",
    "        #'''\n",
    "        #Color Direction\n",
    "        colormap = plt.colormaps.get_cmap('hsv')\n",
    "        # colors = colormap(scaled_z)\n",
    "        sm = plt.cm.ScalarMappable(cmap=colormap)\n",
    "        sm.set_clim(vmin=-3.14, vmax=3.14)\n",
    "        plt.colorbar(sm, ax=ax, shrink=.8, pad=.025)\n",
    "        #'''\n",
    "\n",
    "        '''\n",
    "        #Color Speed\n",
    "        mappable = cm.ScalarMappable(cmap=cm.hsv, norm=norm)\n",
    "        mappable.set_array(speed)\n",
    "        cbar = plt.colorbar(mappable, ax=ax, pad=0.1)\n",
    "        cbar.set_label('Wind Speed')\n",
    "        '''\n",
    "\n",
    "        for i in range(0,len(x),skip):\n",
    "            ax.quiver(x[i], y[i], height/9.81, u[i], v[i], w[i], colors=colors[i], length=1, arrow_length_ratio=.5, normalize=True)\n",
    "\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_zlabel('Height Level')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        plt.title(str(df['timestep'][0]) + \" - \" + str(height/9.81) + \"m\" )\n",
    "\n",
    "        #ax.set_ylim(ax.get_ylim()[::-1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(synth_df, aggregate_df):\n",
    "    synth_df['timestep'] = aggregate_df[\"TIMESTEP\"].unique()[0] #All the data should already be the same timestep\n",
    "    #synth_df.index = synth_df['timestep']\n",
    "    #synth_df = synth_df.drop(columns=['temperature','direction','station_number'])\n",
    "    pres = []\n",
    "    for i in range(0, len(synth_df)):\n",
    "        atm = ATMOSPHERE_1976(Z=synth_df['height'][i])\n",
    "        pres.append(atm.P/100)\n",
    "    synth_df['pressure'] = pres\n",
    "    synth_df['u_wind'] = synth_df['u_wind']/1.94384 # knots to m/s\n",
    "    synth_df['v_wind'] = synth_df['v_wind']/1.94384 # knots to m/s\n",
    "\n",
    "    synth_df['timestep'] = synth_df['timestep'].apply(lambda x: '-'.join([f'{int(part):02d}' for part in x.split('-')]))\n",
    "    synth_df['timestep'] = pd.to_datetime(synth_df['timestep'], format='%Y-%m-%d-%H')\n",
    "\n",
    "    synth_df['height'] = synth_df['height']*9.81 #altitude to geopotential\n",
    "\n",
    "    #synth_df = synth_df.round(2)\n",
    "\n",
    "    #reverse altitude to match xarray structure\n",
    "    synth_df = synth_df.reindex(index=synth_df.index[::-1])\n",
    "    synth_df = synth_df.reset_index(drop=True)\n",
    "\n",
    "    return synth_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georgiy's Method of Synthesizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "base_dir = r\"/mnt/d/RadioWinds/SOUNDINGS_DATA\" \n",
    "df = aggregate_data(base_dir, 1, 0)\n",
    "#print(df)\n",
    "\n",
    "df = df[(df['height'] >= 15000) & (df['height'] <= 28000)] #Select desired altitude ranges\n",
    "df = df.drop('station', axis=1).dropna() #Getting rid of unneeded columns\n",
    "#Analytically selected lat/lon ranges\n",
    "df = df[(df['longitude'] >= 90) & (df['longitude'] <= 131.3) & (df['latitude'] <= 18) &(df['latitude'] >= -12.4)]\n",
    "#df = df[(df['longitude'] >= -135) & (df['longitude'] <= -100) & (df['latitude'] <= 45) &(df['latitude'] >= 15)]\n",
    "\n",
    "lat_min, lat_max = df['latitude'].min(), df['latitude'].max()\n",
    "lon_min, lon_max = df['longitude'].min(), df['longitude'].max()\n",
    "step = 1\n",
    "synth_df = create_synthetic_stations(df, lat_min, lat_max, lon_min, lon_max, step)\n",
    "\n",
    "#Format the SynthDF to match Xarray\n",
    "synth_df = format(synth_df, df)\n",
    "\n",
    "synth_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tristan's Method of Synthesizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Data\n",
    "base_dir = r\"/mnt/d/RadioWinds/SOUNDINGS_DATA\" \n",
    "df2 = aggregate_data2(base_dir, 2023, 8, 3, 12)\n",
    "#df2 = df2[(df2['longitude'] >= 90) & (df2['longitude'] <= 131.3) & (df2['latitude'] <= 18) &(df2['latitude'] >= -12.4)]\n",
    "\n",
    "#Match the Lat/Lon Ranges of the ERA5 subset\n",
    "lat_min, lat_max = 30, 45        # US:  30,    45      SEA:  -12,   18\n",
    "lon_min, lon_max = -125, -100    # US: -125, -100      SEA:   90,  131\n",
    "alt_min, alt_max = 15000, 28000\n",
    "\n",
    "\n",
    "#SEA:\n",
    "#df2 = df2[(df2['longitude'] >= 90) & (df2['longitude'] <= 131.3) & (df2['latitude'] <= 18) &(df2['latitude'] >= -12.4)]\n",
    "\n",
    "#US SOuthwest:\n",
    "df2 = df2[(df2['longitude'] >= lon_min) & (df2['longitude'] <= lon_max) & (df2['latitude'] >= lat_min) & (df2['latitude'] <= lat_max) ]\n",
    "\n",
    "df2 = df2[(df2['height'] >= alt_min) & (df2['height'] <= alt_max)] #Select desired altitude ranges\n",
    "\n",
    "step = .25\n",
    "synth_df2 = create_synthetic_stations2(df2, lat_min, lat_max, lon_min, lon_max, step)\n",
    "\n",
    "#Format the SynthDF to match Xarray\n",
    "synth_df2 = format_df(synth_df2, df2)\n",
    "\n",
    "synth_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(df):\n",
    "\n",
    "    print(df.isnull().values.any())\n",
    "    if df.isnull().values.any():\n",
    "        for idx, r in synth_df2.iterrows():\n",
    "            nulls = list(r[r.isnull()].index)\n",
    "            if nulls:\n",
    "                print(f'row {idx}: those columns are null:', nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(df2)\n",
    "\n",
    "check_missing(synth_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.height.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth Data\n",
    "#lats, lons, alts = get_dims(synth_df)\n",
    "#synth_df = glob_smooth(synth_df, lats, lons, alts)\n",
    "\n",
    "#Smooth Data\n",
    "lats, lons, alts = get_dims(synth_df2)\n",
    "synth_df2 = glob_smooth(synth_df2, lats, lons, alts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(synth_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Georgiy's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 51\n",
    "\n",
    "print( synth_df['height'][index]/9.81)\n",
    "print( synth_df['pressure'][index])\n",
    "plot_layer(synth_df, 'xy', synth_df['height'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Tristan's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altitude = 26250\n",
    "#synth_df2['longitude'] = synth_df2['longitude'].values[::-1]\n",
    "#print(synth_df2['longitude'])\n",
    "#print(synth_df2['latitude'][::-1])\n",
    "plot_layer(synth_df2, 'xy', altitude*9.81, skip =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_original(df2, synth_df2['height'][20]/9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_original(df,  synth_df['height'][20]/9.81)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Simulated NETCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToNETCDF(synth_df):\n",
    "\n",
    "    #reverse longitude\n",
    "    #synth_df['longitude'] = synth_df['longitude'].values[::-1]\n",
    "\n",
    "    alt = synth_df['height'].values#[::-1]\n",
    "    u_wind = synth_df['u_wind'].values\n",
    "    v_wind = synth_df['v_wind'].values\n",
    "    lat = synth_df['latitude'].values\n",
    "    lon = synth_df['longitude'].values#[::-1] #for Some reason this needs to be reversed for Xarray?\n",
    "    level = synth_df['pressure'].values#[::-1]\n",
    "    time = synth_df['timestep'].values\n",
    "    \n",
    "    dims = [len(np.unique(lat)), len(np.unique(lon)), len(np.unique(level)), len(np.unique(time))]\n",
    "    \n",
    "    lat_dim = len(np.unique(lat))\n",
    "    lon_dim = len(np.unique(lon))\n",
    "    level_dim = len(np.unique(level))\n",
    "    time_dim = len(np.unique(time))\n",
    "    \n",
    "    print(\"dims\", lat_dim, lon_dim, level_dim, time_dim)\n",
    "    \n",
    "    alt_split = alt.reshape((time_dim, lat_dim, lon_dim, level_dim))\n",
    "    u_wind_split = u_wind.reshape((time_dim, lat_dim, lon_dim, level_dim))\n",
    "    v_wind_split = v_wind.reshape((time_dim, lat_dim, lon_dim, level_dim))\n",
    "    \n",
    "    print(alt_split.shape)\n",
    "    print(np.transpose(alt_split, (0, 3, 1, 2)).shape)\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'z' : (['time','level','latitude','longitude'], np.transpose(alt_split,(0, 3, 1, 2))),\n",
    "            'u' : (['time','level','latitude','longitude'], np.transpose(u_wind_split, (0, 3, 1, 2))),\n",
    "            'v' : (['time','level','latitude','longitude'], np.transpose(v_wind_split, (0, 3, 1, 2))),\n",
    "        },\n",
    "    \n",
    "        coords={\n",
    "            'time' : np.unique(time),\n",
    "            'level' : np.unique(level),\n",
    "            'latitude' : np.unique(lat)[::-1],\n",
    "            'longitude' : np.unique(lon),\n",
    "            \n",
    "        },\n",
    "    \n",
    "        attrs={\n",
    "            'Conventions': 'CF-1.6'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Reverse Longitude to match ERA5\n",
    "    ds = ds.reindex(longitude=ds.longitude[::-1])\n",
    "    ds['longitude'] = ds['longitude'].reindex(longitude=list(reversed(ds.longitude)))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = convertToNETCDF(synth_df2)\n",
    "ds.to_netcdf(r'/home/schuler/FLOW2D/forecasts/SynthCast.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs in each variable\n",
    "nan_check = ds.map(lambda x: np.isnan(x).any())\n",
    "\n",
    "print(nan_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Synth Winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = None\n",
    "ds = xr.open_dataset(r'/home/schuler/FLOW2D/forecasts/SynthCast.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from era5.forecast import Forecast, Forecast_Subset\n",
    "from era5.forecast_visualizer import ForecastVisualizer\n",
    "\n",
    "filename = \"../../forecasts/SynthCast.nc\"\n",
    "FORECAST_PRIMARY = Forecast(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_subset = Forecast_Subset(FORECAST_PRIMARY)\n",
    "forecast_subset.assign_coord(0.5*(forecast_subset.Forecast.LAT_MAX + forecast_subset.Forecast.LAT_MIN),\n",
    "                              0.5*(forecast_subset.Forecast.LON_MAX + forecast_subset.Forecast.LON_MIN),\n",
    "                                \"2023-08-03T12:00:00.000000000\")\n",
    "forecast_subset.subset_forecast()\n",
    "\n",
    "#forecast_subset.ds = forecast_subset.ds.isel(level = slice(10,11))\n",
    "\n",
    "#Heights for Comparing\n",
    "print(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "\n",
    "avg_alt = np.average(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "print(avg_alt)\n",
    "#Round to nearest 250\n",
    "avg_alt = int(avg_alt/ 250) * 250\n",
    "print(avg_alt)\n",
    "\n",
    "forecast_subset.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_visualizer = ForecastVisualizer(forecast_subset)\n",
    "forecast_visualizer.generate_flow_array(timestamp = forecast_subset.ds.time.values[0])\n",
    "\n",
    "# Initialize Figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "#ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1 = fig.add_subplot(111, projection='custom3dquiver')\n",
    "# Manually add a CustomAxes3D to the figure\n",
    "#ax1 = Custom3DQuiver(fig)\n",
    "fig.add_axes(ax1)\n",
    "\n",
    "skip = 1\n",
    "\n",
    "#print(\"Saving Figure \" + str(timestamp))\n",
    "forecast_visualizer.visualize_3d_planar_flow(ax1, skip)\n",
    "#plt.savefig(str(i) +'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERA5 Forecast Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from era5.forecast import Forecast, Forecast_Subset\n",
    "from era5.forecast_visualizer import ForecastVisualizer\n",
    "\n",
    "filename = \"../../forecasts/Jan-2023-SEA.nc\"\n",
    "FORECAST_PRIMARY = Forecast(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_subset = Forecast_Subset(FORECAST_PRIMARY)\n",
    "forecast_subset.assign_coord(0.5*(forecast_subset.Forecast.LAT_MAX + forecast_subset.Forecast.LAT_MIN),\n",
    "                              0.5*(forecast_subset.Forecast.LON_MAX + forecast_subset.Forecast.LON_MIN),\n",
    "                                \"2023-01-01T00:00:00.000000000\")\n",
    "forecast_subset.subset_forecast()\n",
    "\n",
    "forecast_subset.ds = forecast_subset.ds.isel(level = slice(1,2))\n",
    "\n",
    "#Heights for Comparing\n",
    "print(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "\n",
    "avg_alt = np.average(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "print(avg_alt)\n",
    "#Round to nearest 250\n",
    "avg_alt = int(avg_alt/ 250) * 250\n",
    "print(avg_alt)\n",
    "\n",
    "forecast_subset.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_visualizer = ForecastVisualizer(forecast_subset)\n",
    "forecast_visualizer.generate_flow_array(timestamp = forecast_subset.ds.time.values[0])\n",
    "\n",
    "# Initialize Figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "#ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1 = fig.add_subplot(111, projection='custom3dquiver')\n",
    "# Manually add a CustomAxes3D to the figure\n",
    "#ax1 = Custom3DQuiver(fig)\n",
    "fig.add_axes(ax1)\n",
    "\n",
    "skip = 5\n",
    "\n",
    "#print(\"Saving Figure \" + str(timestamp))\n",
    "forecast_visualizer.visualize_3d_planar_flow(ax1, skip)\n",
    "#plt.savefig(str(i) +'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "alts = []\n",
    "\n",
    "slices = 6\n",
    "\n",
    "for i in range (slices):\n",
    "    #Reset Forecast Subset \n",
    "\n",
    "    forecast_subset.assign_coord(0.5*(forecast_subset.Forecast.LAT_MAX + forecast_subset.Forecast.LAT_MIN),\n",
    "                              0.5*(forecast_subset.Forecast.LON_MAX + forecast_subset.Forecast.LON_MIN),\n",
    "                                \"2023-01-01T00:00:00.000000000\");\n",
    "    forecast_subset.subset_forecast();\n",
    "    #Choose new slice\n",
    "    forecast_subset.ds = forecast_subset.ds.isel(level = slice(i,i+1))\n",
    "\n",
    "    #Regenerate Forecast Visualizer\n",
    "    forecast_visualizer = ForecastVisualizer(forecast_subset);\n",
    "    forecast_visualizer.generate_flow_array(timestamp = forecast_subset.ds.time.values[0]);\n",
    "\n",
    "\n",
    "    #Get alt info for comparison\n",
    "    avg_alt = np.average(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "    #Round to nearest 250\n",
    "    avg_alt = int(avg_alt/ 250) * 250\n",
    "    #Append\n",
    "    alts.append(avg_alt)\n",
    "\n",
    "\n",
    "    # Initialize Figure\n",
    "    fig = plt.figure(figsize=(10, 8));\n",
    "    #ax1 = fig.add_subplot(111, projection='3d')\n",
    "    ax1 = fig.add_subplot(111, projection='custom3dquiver');\n",
    "    # Manually add a CustomAxes3D to the figure\n",
    "    #ax1 = Custom3DQuiver(fig)\n",
    "    fig.add_axes(ax1);\n",
    "    \n",
    "    skip = 5\n",
    "    \n",
    "    #print(\"Saving Figure \" + str(timestamp))\n",
    "    forecast_visualizer.visualize_3d_planar_flow(ax1, skip);\n",
    "\n",
    "    print(\"Saving Figure Level \" + str(forecast_subset.ds.isel(level = 0)))\n",
    "    plt.savefig(str(i) +'.png');\n",
    "\n",
    "with imageio.get_writer('ERA5-Slices.gif', mode='I', duration=1000, loop=0) as writer:\n",
    "    for i in range(slices):\n",
    "        image = imageio.imread(str(i) +'.png')\n",
    "        print(str(i) +'.png')\n",
    "        writer.append_data(image)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alts)\n",
    "# Path to your GIF file\n",
    "gif_path = 'ERA5-Slices.gif'\n",
    "# Display the GIF\n",
    "Image(filename=gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i in range(0,len(alts)):\n",
    "    altitude = alts[i]\n",
    "    plot_layer(synth_df2, 'xy', altitude*9.81);\n",
    "\n",
    "    plt.savefig(\"Radiosonde-\" + str(i) +'.png')\n",
    "\n",
    "with imageio.get_writer('Radiosonde-Slices.gif', mode='I', duration=1000, loop=0) as writer:\n",
    "    for i in range(slices):\n",
    "        image = imageio.imread(\"Radiosonde-\" + str(i) +'.png')\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your GIF file\n",
    "gif_path = 'Radiosonde-Slices.gif'\n",
    "# Display the GIF\n",
    "Image(filename=gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bigger Synthetic Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATE: 2023 1 1 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 1 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 2 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 2 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 3 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 3 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 50 1\n",
      "(1, 121, 133, 50)\n",
      "(1, 50, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 4 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 4 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 5 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 5 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 6 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 52 1\n",
      "(1, 121, 133, 52)\n",
      "(1, 52, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 6 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 53 1\n",
      "(1, 121, 133, 53)\n",
      "(1, 53, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 7 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 121 133 52 1\n",
      "(1, 121, 133, 52)\n",
      "(1, 52, 121, 133)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 1 7 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n"
     ]
    }
   ],
   "source": [
    "year = 2023\n",
    "month = 1\n",
    "day =1\n",
    "\n",
    "ds_list = []\n",
    "\n",
    "for d in range(day, 31+1):\n",
    "    for h in [0,12]:\n",
    "\n",
    "        print()\n",
    "        print(\"DATE:\", year,month,d,h)\n",
    "\n",
    "        # Aggregate Data\n",
    "        base_dir = r\"/mnt/d/RadioWinds/SOUNDINGS_DATA\" \n",
    "        df2 = aggregate_data2(base_dir, year, month, d, h)\n",
    "\n",
    "        #Match the Lat/Lon Ranges of the ERA5 subset\n",
    "        lat_min, lat_max = -12,   18       # US:  30,    45      SEA:  -12,   18\n",
    "        lon_min, lon_max = 98,  131    # US: -125, -100      SEA:   98,  131\n",
    "        alt_min, alt_max = 15000, 28000\n",
    "        #New Pressure levels for ERA5 download is 10-200hpa\n",
    "        \n",
    "        \n",
    "        #SEA:\n",
    "        #df2 = df2[(df2['longitude'] >= 90) & (df2['longitude'] <= 132) & (df2['latitude'] <= 18) &(df2['latitude'] >= -12.4)]\n",
    "\n",
    "        #Smaller SEA region:\n",
    "        \n",
    "        \n",
    "        #US SOuthwest:\n",
    "        df2 = df2[(df2['longitude'] >= lon_min) & (df2['longitude'] <= lon_max) & (df2['latitude'] >= lat_min) & (df2['latitude'] <= lat_max) ]\n",
    "        \n",
    "        df2 = df2[(df2['height'] >= alt_min) & (df2['height'] <= alt_max)] #Select desired altitude ranges\n",
    "        \n",
    "        step = .25\n",
    "        synth_df2 = create_synthetic_stations2(df2, lat_min, lat_max, lon_min, lon_max, step)\n",
    "                \n",
    "        #Format the SynthDF to match Xarray\n",
    "        synth_df2 = format_df(synth_df2, df2)\n",
    "\n",
    "        check_missing(df2)\n",
    "        check_missing(synth_df2)\n",
    "        \n",
    "        #Smooth Data\n",
    "        lats, lons, alts = get_dims(synth_df2)\n",
    "        synth_df2 = glob_smooth(synth_df2, lats, lons, alts)\n",
    "\n",
    "        check_missing(synth_df2)\n",
    "    \n",
    "        #Convert to Xarray\n",
    "        ds = convertToNETCDF(synth_df2)\n",
    "\n",
    "        # Check for NaNs in each variable\n",
    "        nan_check = ds.map(lambda x: np.isnan(x).any())\n",
    "        \n",
    "        print(nan_check)\n",
    "    \n",
    "        #Add to ds list\n",
    "        ds_list.append(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Determine the global min and max levels\n",
    "all_levels = [ds.level.values for ds in ds_list]\n",
    "min_level = max(l.min() for l in all_levels)\n",
    "max_level = min(l.max() for l in all_levels)\n",
    "\n",
    "print(all_levels)\n",
    "print(min_level)\n",
    "print(max_level)\n",
    "\n",
    "\n",
    "ds_list_formatted = []\n",
    "\n",
    "for ds in ds_list:\n",
    "    ds_list_formatted.append(ds.sel(level=slice(min_level, max_level)))\n",
    "\n",
    "print(ds_list_formatted)\n",
    "\n",
    "# Concatenate along the 'level' dimension\n",
    "synthetic_forecast = xr.concat(ds_list_formatted, dim='time')\n",
    "\n",
    "print(synthetic_forecast)\n",
    "\n",
    "\n",
    "#for d in standardized_data_arrays:\n",
    "#    print(d.dims)\n",
    "#    print(d.z[0,:,0,0])\n",
    "\n",
    "\n",
    "# Check for NaNs in each variable\n",
    "nan_check = synthetic_forecast.map(lambda x: np.isnan(x).any())\n",
    "\n",
    "print(nan_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to Netcdf\n",
    "synthetic_forecast.to_netcdf(r'/home/schuler/FLOW2D/forecasts/SYNTH-Aug-2023-SEA.nc')\n",
    "synthetic_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs in each variable\n",
    "nan_check = synthetic_forecast.map(lambda x: np.isnan(x).any())\n",
    "\n",
    "print(nan_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( synthetic_forecast.z[5,:,0,0]/9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
