{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from fluids import ATMOSPHERE_1976\n",
    "\n",
    "import xarray as xr \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib import cm\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import re\n",
    "#matplotlib notebook\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(base_dir, month, day, write=False):\n",
    "\n",
    "    \"\"\"With a folder containing multiple station's worth of Radiosonde data, create a concatenated CSV of all radiosonde data from the \n",
    "    given day of the given month.\\\\\n",
    "    \n",
    "    base_dir: Directory containing all station data folders.\\\\\n",
    "    month: integer from 1-12\\\\\n",
    "    day: integer from 1-x, based on which month you select\"\"\"\n",
    "\n",
    "    print('AGGREGATION... ')\n",
    "\n",
    "    csv_list = []\n",
    "\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder+'/2023/') #Can modify this for the year\n",
    "\n",
    "        folder_path = os.path.join(folder_path, str(month)) #create a path to the folder for the month you want data from\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "                #Finds every csv in the selected month folder\n",
    "                csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "                if csv_files:\n",
    "                    # Read the X-th (x is the day you choose) CSV file in the selected folder\n",
    "                    first_csv_path = os.path.join(folder_path, csv_files[day])\n",
    "                    \n",
    "                    # Append the path to the list\n",
    "                    csv_list.append(first_csv_path)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through the csv_list and read each CSV file\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        #create a new column for the date and time of the reading\n",
    "        datetime = csv_path.split('/')[-1].split('.')[0].split('-')[1:]\n",
    "        timestep = '-'.join(datetime)\n",
    "        df['TIMESTEP'] = timestep\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all the DataFrames\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True).drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file if write == True\n",
    "    if write == True: \n",
    "        output_csv_path = '/path/to/save/concatenated.csv'\n",
    "        concatenated_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print('AGGREGATION : DONE.')\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data2(base_dir, year, month, day, hour, write=False):\n",
    "\n",
    "    \"\"\"With a folder containing multiple station's worth of Radiosonde data, create a concatenated CSV of all radiosonde data from the \n",
    "    given day of the given month.\\\\\n",
    "    \n",
    "    base_dir: Directory containing all station data folders.\\\\\n",
    "    month: integer from 1-12\\\\\n",
    "    day: integer from 1-x, based on which month you select\"\"\"\n",
    "\n",
    "    print('AGGREGATION... ')\n",
    "\n",
    "    csv_list = []\n",
    "\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder+'/' + str(year) + '/') \n",
    "\n",
    "        folder_path = os.path.join(folder_path, str(month)) \n",
    "\n",
    "        #Iterate through every station for a particular month and year (not necessarily alphabetical order)\n",
    "        if os.path.isdir(folder_path):\n",
    "                #Check if the station has data for that month\n",
    "                csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "                if csv_files:\n",
    "                    #Parse out the station info for the CSV\n",
    "                    string_parse_pattern = r'^[^-]+-[^-]+-'\n",
    "                    # Search the pattern in the string\n",
    "                    station_info = re.match(string_parse_pattern, csv_files[0])\n",
    "                    station_info = station_info.group(0)\n",
    "\n",
    "                    #Create the correct csv string name for year, month, day, and time for that particular station\n",
    "                    selected_csv  = os.path.join(folder_path, station_info + str(month) + '-' + str(day) + '-' + str(hour) + '.csv')\n",
    "\n",
    "                    #If the CSV exists, add it to the array for parsing, otherwise skip.\n",
    "                    if os.path.isfile(selected_csv):\n",
    "                        # Append the path to the list\n",
    "                        csv_list.append(selected_csv)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through the csv_list and read each CSV file\n",
    "    for csv_path in csv_list:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        #create a new column for the date and time of the reading\n",
    "        datetime = csv_path.split('/')[-1].split('.')[0].split('-')[1:]\n",
    "        timestep = '-'.join(datetime)\n",
    "        df['TIMESTEP'] = timestep\n",
    "\n",
    "\n",
    "        #THIS IS ALL NEW PREPROCESSING STUFF DURRING AGGREGATION\n",
    "\n",
    "        # Drop rows where 'height' is NaN\n",
    "        df = df.dropna(subset=['height'])\n",
    "        # Create a new column for the rounded heights to 250m\n",
    "        df['rounded_height'] = df['height'].apply(lambda x: round(x / 250) * 250)\n",
    "\n",
    "        #df['rounded_pressure'] = df['height'].apply(lambda x: round(x / 250) * 250)\n",
    "        \n",
    "        # Calculate the absolute difference between original and rounded heights\n",
    "        df['abs_diff'] = abs(df['height'] - df['rounded_height'])\n",
    "\n",
    "        # Sort by rounded height and absolute difference\n",
    "        df = df.sort_values(by=['rounded_height', 'abs_diff'])\n",
    "\n",
    "        # Drop duplicates, keeping the row with the smallest difference\n",
    "        df = df.drop_duplicates(subset='rounded_height', keep='first')\n",
    "\n",
    "        \n",
    "\n",
    "        # Create a new DataFrame with the desired height intervals\n",
    "        intervals = np.arange(0, 32001, 250)  # Heights from 0 to 30000 at 250m intervals\n",
    "        df_intervals = pd.DataFrame({'rounded_height': intervals})\n",
    "\n",
    "        # Merge the rounded DataFrame with the new intervals\n",
    "        df_resampled = df_resampled = pd.merge(df_intervals, df, on='rounded_height', how='left')\n",
    "\n",
    "        df_resampled['height'] = df_resampled['rounded_height']\n",
    "        df_resampled = df_resampled.drop(columns=['rounded_height', 'abs_diff'])\n",
    "\n",
    "        #print(df_resampled)\n",
    "\n",
    "        df_list.append(df_resampled)\n",
    "\n",
    "    # Concatenate all the DataFrames\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True).drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "    # Save the concatenated DataFrame to a new CSV file if write == True\n",
    "    if write == True: \n",
    "        output_csv_path = '/path/to/save/concatenated.csv'\n",
    "        concatenated_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "    concatenated_df = concatenated_df[['pressure','height', 'direction', 'speed', 'u_wind', 'v_wind', 'time', 'latitude', 'longitude', 'TIMESTEP']]\n",
    "\n",
    "    #print(concatenated_df)\n",
    "    #print(concatenated_df.height.max())\n",
    "    # Drop rows where where there are any nan values\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    \n",
    "    print('AGGREGATION : DONE.')\n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_level_original(df, altitude):\n",
    "    \n",
    "    df= df[(df['height'] >=  altitude-125)& (df['height'] <= altitude+125)]\n",
    "\n",
    "    x = df['longitude'].values\n",
    "    y = df['latitude'].values\n",
    "    z = df['height'].values\n",
    "    u = df['u_wind'].values\n",
    "    v = df['v_wind'].values\n",
    "    w = np.zeros_like(u)\n",
    "    speed= df['speed']#.astype('int')\n",
    "    \n",
    "    norm = plt.Normalize(speed.min(), speed.max())\n",
    "    colors = cm.hsv(norm(speed))\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = fig.add_subplot(111, projection = '3d')\n",
    "    mappable = cm.ScalarMappable(cmap=cm.hsv, norm=norm)\n",
    "    mappable.set_array(speed)\n",
    "    cbar = plt.colorbar(mappable, ax=ax, fraction = 0.03, pad=0.1)\n",
    "    cbar.set_label('Wind Speed')\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        ax.quiver(x[i], y[i], z[i], u[i], v[i], w[i], colors=colors[i], length=2, arrow_length_ratio=0.5, normalize=True)\n",
    "    \n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_zlabel('Pressure Level')\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_stations(df, lat_min, lat_max, lon_min, lon_max, step, alt_step=250):\n",
    "\n",
    "    \"\"\"Use: Create a (optionally) high resolution 3-D grid of radiosonde data within a specific longitude-latitude and altitude range, \n",
    "    provided you have prior data to synthesize from.\\\\\n",
    "    \n",
    "    df: Dataframe produced by aggregate_data function, based on existing radiosonde data.\\\\\n",
    "    lat_min: minimum latitude of region determined by df.\\\\\n",
    "    lat_max: maximum latitude of region determined by df.\\\\\n",
    "    lon_min: minimum longitude of region determined by df.\\\\\n",
    "    lon_max: maximum longitude of region determined by df.\\\\\n",
    "    step: resolution in latitude-longitude for which to generate data. (Ex. step = 1 means data will be generated each 1 lat-lon degrees).\\\\\n",
    "    alt_step: Vertical resolution in meters for which to generate data. (Ex. alt_step = 250 means data will be generated every 250 meters.)\n",
    "    \"\"\"\n",
    "\n",
    "    print('SYNTHESIZING...')\n",
    "\n",
    "    #Creates a range of lats/lons used for coordiantes when making data.\n",
    "    latitudes = np.arange(lat_min, lat_max + step, step) \n",
    "    longitudes = np.arange(lon_min, lon_max + step, step)\n",
    "    \n",
    "    # Define altitude range based on the min and max altitudes in the original data\n",
    "    alt_min, alt_max = df['height'].min(), df['height'].max()\n",
    "    altitudes = np.arange(alt_min, alt_max + alt_step, alt_step)\n",
    "    \n",
    "    synthetic_data = []\n",
    "\n",
    "    station_number = 0 \n",
    "\n",
    "    for lat in latitudes:\n",
    "        for lon in longitudes:\n",
    "            station_number += 1\n",
    "            # Interpolate data for each altitude level\n",
    "            for altitude in altitudes:\n",
    "                #Collect data into layers based on alt_step distance. \n",
    "                df_alt = df[(df['height'] >= altitude - alt_step/2) & (df['height'] < altitude + alt_step/2)]\n",
    "                if df_alt.empty:\n",
    "                    continue\n",
    "                \n",
    "                #Collect all features corresponding to selected lat/lon ranges. \n",
    "                points = df_alt[['latitude', 'longitude']].values\n",
    "                columns = ['pressure','height','temperature','direction','speed','u_wind','v_wind']\n",
    "                \n",
    "                interpolated_values = {}\n",
    "                for column in columns:\n",
    "                    values = df_alt[column].values\n",
    "                    #Utilize Nearest neighbor interpolation to fill empty spaces on 3d grid. \n",
    "                    interpolated_values[column] = griddata(points, values, (lat, lon), method='nearest')\n",
    "\n",
    "                synthetic_data.append({\n",
    "                    **interpolated_values,\n",
    "                    'station_number': station_number,\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'height': altitude\n",
    "                })\n",
    "    \n",
    "    synthetic_df = pd.DataFrame(synthetic_data)\n",
    "    print('SYNTHESIS : DONE.')\n",
    "    return synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_stations2(df, lat_min, lat_max, lon_min, lon_max, step, alt_step=250):\n",
    "\n",
    "    \"\"\"Use: Create a (optionally) high resolution 3-D grid of radiosonde data within a specific longitude-latitude and altitude range, \n",
    "    provided you have prior data to synthesize from.\\\\\n",
    "    \n",
    "    df: Dataframe produced by aggregate_data function, based on existing radiosonde data.\\\\\n",
    "    lat_min: minimum latitude of region determined by df.\\\\\n",
    "    lat_max: maximum latitude of region determined by df.\\\\\n",
    "    lon_min: minimum longitude of region determined by df.\\\\\n",
    "    lon_max: maximum longitude of region determined by df.\\\\\n",
    "    step: resolution in latitude-longitude for which to generate data. (Ex. step = 1 means data will be generated each 1 lat-lon degrees).\\\\\n",
    "    alt_step: Vertical resolution in meters for which to generate data. (Ex. alt_step = 250 means data will be generated every 250 meters.)\n",
    "    \"\"\"\n",
    "\n",
    "    print('SYNTHESIZING...')\n",
    "\n",
    "    #Creates a range of lats/lons used for coordiantes when making data.\n",
    "    latitudes = np.arange(lat_min, lat_max + step, step) \n",
    "    longitudes = np.arange(lon_min, lon_max + step, step)\n",
    "    lat_grid, lon_grid = np.meshgrid(latitudes, longitudes)\n",
    "\n",
    "    \n",
    "    # Define altitude range based on the min and max altitudes in the original data\n",
    "    alt_min, alt_max = df['height'].min(), df['height'].max()\n",
    "    altitudes = np.arange(alt_min, alt_max + alt_step, alt_step)\n",
    "    \n",
    "    synthetic_data = []\n",
    "\n",
    "    station_number = 0 \n",
    "\n",
    "\n",
    "    for altitude in altitudes:\n",
    "        #Collect data into layers based on alt_step distance. \n",
    "        df_alt = df[(df['height'] == altitude)]\n",
    "\n",
    "        #Check for missing data for interpolating at a particular altitude level\n",
    "        df_alt = df_alt.dropna(subset=['u_wind'])\n",
    "        \n",
    "        if df_alt.empty:\n",
    "            continue\n",
    "        \n",
    "        #Collect all features corresponding to selected lat/lon ranges. \n",
    "        points = df_alt[['latitude', 'longitude']].values\n",
    "        variables = {\n",
    "            'pressure': df_alt['pressure'].values,\n",
    "            'height': df_alt['height'].values,\n",
    "            #'temperature': df_alt['temperature'].values,\n",
    "            'direction': df_alt['direction'].values,\n",
    "            'speed': df_alt['speed'].values,\n",
    "            'u_wind': df_alt['u_wind'].values,\n",
    "            'v_wind': df_alt['v_wind'].values,\n",
    "        }\n",
    "\n",
    "\n",
    "        #Create initial shape\n",
    "        interpolated_values2 = griddata(points, variables['height'], (lat_grid, lon_grid), method='nearest')\n",
    "        df_interpolated = pd.DataFrame( interpolated_values2, index=longitudes, columns=latitudes)\n",
    "        df_interpolated = df_interpolated.reset_index()\n",
    "        df_interpolated = df_interpolated.melt(id_vars='index', var_name='latitude', value_name='height')\n",
    "        df_interpolated = df_interpolated.rename(columns={'index': 'longitude'})\n",
    "\n",
    "        # Perform interpolation\n",
    "        for var_name, values in variables.items():\n",
    "\n",
    "            #grid_values = griddata(points, values, (lat_grid, lon_grid), method='nearest')\n",
    "            interpolated_values = griddata(points, values, (lat_grid, lon_grid), method='nearest')\n",
    "\n",
    "            # Convert grid values to DataFrame\n",
    "            df_interpolated_var = pd.DataFrame( interpolated_values, index=longitudes, columns=latitudes)\n",
    "            df_interpolated_var = df_interpolated_var.reset_index()\n",
    "            df_interpolated_var = df_interpolated_var.melt(id_vars='index', var_name='latitude', value_name=var_name)\n",
    "            df_interpolated_var = df_interpolated_var.rename(columns={'index': 'longitude'})\n",
    "\n",
    "            df_interpolated[var_name] = df_interpolated_var[var_name]\n",
    "            \n",
    "            # Reorder columns\n",
    "            #df_interpolated = df_interpolated[['latitude', 'longitude', 'altitude']]\n",
    "            \n",
    "\n",
    "        synthetic_data.append(df_interpolated)\n",
    "\n",
    "    # Concatenate all the DataFrames\n",
    "    concatenated_df = pd.concat(synthetic_data, ignore_index=True)\n",
    "\n",
    "    # Sort by latitude, longitude, and altitude\n",
    "    concatenated_df = concatenated_df.sort_values(by=['latitude', 'longitude', 'height']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \n",
    "    print('SYNTHESIS : DONE.')\n",
    "    return concatenated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, lats, lons, sigma):\n",
    "\n",
    "    \"\"\"Use: Helper function that will smooth a variable on a single altitude layer.\\\\\n",
    "    x: list-like of variables needing to be smoothed.\\\\\n",
    "    lats: latitude dimension (Number of unique latitude values in data).\\\\\n",
    "    lons: longitude dimension (Number of unique longitude values in data).\\\\\n",
    "    sigma: Standard deviation of guassian kernel (Higher value = more homogenous result overall).\"\"\"\n",
    "\n",
    "    x = np.array(x).reshape((lats,lons)) #Reshape data into a matrix matching the geographic grid. \n",
    "    x1 = gaussian_filter(x, sigma=sigma) #apply smoothing across the matrix. \n",
    "    x1 = np.concatenate(x1, axis=0) #concatenate back to a numpy array for compatibility with later functions. \n",
    "\n",
    "    return x1\n",
    "\n",
    "def get_dims(df):\n",
    "    lats = df['latitude'].nunique()\n",
    "    lons = df['longitude'].nunique()\n",
    "    alts = df['height'].nunique()\n",
    "\n",
    "    return (lats, lons, alts)\n",
    "\n",
    "\n",
    "def glob_smooth(df, lats, lons, alts):\n",
    "\n",
    "    \"\"\"Use: With a synthetic dataframe produced by the create_synthetic_stations function, apply a gaussian smoothing \n",
    "    to each altitude layer and re-stack in order to produce a complete dtaframe of synthetic winds.\\\n",
    "    \n",
    "    df: Synthetic dataframe from create_synthetic_stations.\\\\\n",
    "    lats: Dimensions of latitude data. (Ex. Data has a latitude range of 70-110; lats = 40)\\\\\n",
    "    lons: Dimensions of longitude data. (Ex. Same logic as latitudes)\\\\\n",
    "    alts: Dimensions of altitude data. (Ex. Number of altitude levels in data) \"\"\"\n",
    "    \n",
    "    print(\"SMOOTHING...\")\n",
    "\n",
    "    speed = []\n",
    "    u = []\n",
    "    v = []\n",
    "    for i in range(0,alts): #For eahc altitude layer\n",
    "        for j in range(0,lats*lons): #For each station in the latitude-longitude grid \n",
    "            idx = alts * j + i #Each station index\n",
    "            speed.append(df['speed'][idx]) #Add data to a list for processing\n",
    "            u.append(df['u_wind'][idx])\n",
    "            v.append(df['v_wind'][idx])\n",
    "\n",
    "\n",
    "        #Apply helper function for smoothing all desired variables. \n",
    "        speed1 = smooth(speed, lats, lons, 3)\n",
    "        u1 = smooth(u, lats, lons, 12) # divide by res. so for .25 res is 12, for 1 degree is 3\n",
    "        v1 = smooth(v, lats, lons, 12)\n",
    "\n",
    "        for j in range(0, lats*lons): #Re-create a dataframe based on structure of origina ldata with synthetic values. \n",
    "            idx = alts * j + i\n",
    "            df.at[idx, 'speed'] = speed1[j]\n",
    "            df.at[idx, 'u_wind'] = u1[j]\n",
    "            df.at[idx, 'v_wind'] = v1[j]\n",
    "\n",
    "        speed.clear() #clear lists for use in next altitude layer. \n",
    "        u.clear()\n",
    "        v.clear()\n",
    "\n",
    "    \n",
    "    print('SMOOTHING : DONE.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer(df, type, height:int, skip = 1):\n",
    "    if height not in df['height'].unique():\n",
    "        raise ValueError(\"Invalid height value\")\n",
    "\n",
    "    if type == 'xy':\n",
    "        x = df[df['height'] == height ]['longitude'].values\n",
    "        y = df[df['height'] == height]['latitude'].values\n",
    "        u = df[df['height'] == height]['u_wind'].values\n",
    "        v = df[df['height'] == height]['v_wind'].values\n",
    "        w = np.zeros_like(u)\n",
    "        speed = df[df['height'] == height]['speed'].values\n",
    "\n",
    "        '''\n",
    "        u = u.astype(float)\n",
    "        v = v.astype(float)\n",
    "        print( u)\n",
    "        speed = np.sqrt(u**2 + v**2)\n",
    "        '''\n",
    "        #speed = speed/1.94\n",
    "        #speed = speed.astype('int')\n",
    "\n",
    "        u = np.asarray(u.tolist())\n",
    "        v = np.asarray(v.tolist())\n",
    "\n",
    "        # Calculate directions for color mapping\n",
    "        directions = np.arctan2(v, u)\n",
    "        speed = np.sqrt(v**2 + u**2)/1.94\n",
    "\n",
    "        # For Speed\n",
    "        #norm = plt.Normalize(np.min(speed), np.max(speed))\n",
    "        #colors = cm.hsv(norm(speed))\n",
    "\n",
    "        # For Direction\n",
    "        norm = plt.Normalize(-np.pi, np.pi)\n",
    "        colors = cm.hsv(norm(directions)) #for Directions\n",
    "        res = 1\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "\n",
    "        #'''\n",
    "        #Color Direction\n",
    "        colormap = plt.colormaps.get_cmap('hsv')\n",
    "        # colors = colormap(scaled_z)\n",
    "        sm = plt.cm.ScalarMappable(cmap=colormap)\n",
    "        sm.set_clim(vmin=-3.14, vmax=3.14)\n",
    "        plt.colorbar(sm, ax=ax, shrink=.8, pad=.025)\n",
    "        #'''\n",
    "\n",
    "        '''\n",
    "        #Color Speed\n",
    "        mappable = cm.ScalarMappable(cmap=cm.hsv, norm=norm)\n",
    "        mappable.set_array(speed)\n",
    "        cbar = plt.colorbar(mappable, ax=ax, pad=0.1)\n",
    "        cbar.set_label('Wind Speed')\n",
    "        '''\n",
    "\n",
    "        for i in range(0,len(x),skip):\n",
    "            ax.quiver(x[i], y[i], height/9.81, u[i], v[i], w[i], colors=colors[i], length=1, arrow_length_ratio=.5, normalize=True)\n",
    "\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_zlabel('Height Level')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        plt.title(str(df['timestep'][0]) + \" - \" + str(height/9.81) + \"m\" )\n",
    "\n",
    "        #ax.set_ylim(ax.get_ylim()[::-1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(synth_df, aggregate_df):\n",
    "    synth_df['timestep'] = aggregate_df[\"TIMESTEP\"].unique()[0] #All the data should already be the same timestep\n",
    "    #synth_df.index = synth_df['timestep']\n",
    "    #synth_df = synth_df.drop(columns=['temperature','direction','station_number'])\n",
    "    pres = []\n",
    "    for i in range(0, len(synth_df)):\n",
    "        atm = ATMOSPHERE_1976(Z=synth_df['height'][i])\n",
    "        pres.append(atm.P/100)\n",
    "    synth_df['pressure'] = pres\n",
    "    synth_df['u_wind'] = synth_df['u_wind']/1.94384 # knots to m/s\n",
    "    synth_df['v_wind'] = synth_df['v_wind']/1.94384 # knots to m/s\n",
    "\n",
    "    synth_df['timestep'] = synth_df['timestep'].apply(lambda x: '-'.join([f'{int(part):02d}' for part in x.split('-')]))\n",
    "    synth_df['timestep'] = pd.to_datetime(synth_df['timestep'], format='%Y-%m-%d-%H')\n",
    "\n",
    "    synth_df['height'] = synth_df['height']*9.81 #altitude to geopotential\n",
    "\n",
    "    #synth_df = synth_df.round(2)\n",
    "\n",
    "    #reverse altitude to match xarray structure\n",
    "    synth_df = synth_df.reindex(index=synth_df.index[::-1])\n",
    "    synth_df = synth_df.reset_index(drop=True)\n",
    "\n",
    "    return synth_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Georgiy's Method of Synthesizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "base_dir = r\"/mnt/d/RadioWinds/SOUNDINGS_DATA\" \n",
    "df = aggregate_data(base_dir, 1, 0)\n",
    "#print(df)\n",
    "\n",
    "df = df[(df['height'] >= 15000) & (df['height'] <= 28000)] #Select desired altitude ranges\n",
    "df = df.drop('station', axis=1).dropna() #Getting rid of unneeded columns\n",
    "#Analytically selected lat/lon ranges\n",
    "df = df[(df['longitude'] >= 90) & (df['longitude'] <= 131.3) & (df['latitude'] <= 18) &(df['latitude'] >= -12.4)]\n",
    "#df = df[(df['longitude'] >= -135) & (df['longitude'] <= -100) & (df['latitude'] <= 45) &(df['latitude'] >= 15)]\n",
    "\n",
    "lat_min, lat_max = df['latitude'].min(), df['latitude'].max()\n",
    "lon_min, lon_max = df['longitude'].min(), df['longitude'].max()\n",
    "step = 1\n",
    "synth_df = create_synthetic_stations(df, lat_min, lat_max, lon_min, lon_max, step)\n",
    "\n",
    "#Format the SynthDF to match Xarray\n",
    "synth_df = format(synth_df, df)\n",
    "\n",
    "synth_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tristan's Method of Synthesizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Data\n",
    "base_dir = r\"/mnt/d/RadioWinds/SOUNDINGS_DATA\" \n",
    "df2 = aggregate_data2(base_dir, 2023, 8, 3, 12)\n",
    "#df2 = df2[(df2['longitude'] >= 90) & (df2['longitude'] <= 131.3) & (df2['latitude'] <= 18) &(df2['latitude'] >= -12.4)]\n",
    "\n",
    "#Match the Lat/Lon Ranges of the ERA5 subset\n",
    "lat_min, lat_max = 30, 45        # US:  30,    45      SEA:  -12,   18\n",
    "lon_min, lon_max = -125, -100    # US: -125, -100      SEA:   90,  131\n",
    "alt_min, alt_max = 15000, 28000\n",
    "\n",
    "\n",
    "#SEA:\n",
    "#df2 = df2[(df2['longitude'] >= 90) & (df2['longitude'] <= 131.3) & (df2['latitude'] <= 18) &(df2['latitude'] >= -12.4)]\n",
    "\n",
    "#US SOuthwest:\n",
    "df2 = df2[(df2['longitude'] >= lon_min) & (df2['longitude'] <= lon_max) & (df2['latitude'] >= lat_min) & (df2['latitude'] <= lat_max) ]\n",
    "\n",
    "df2 = df2[(df2['height'] >= alt_min) & (df2['height'] <= alt_max)] #Select desired altitude ranges\n",
    "\n",
    "step = .25\n",
    "synth_df2 = create_synthetic_stations2(df2, lat_min, lat_max, lon_min, lon_max, step)\n",
    "\n",
    "#Format the SynthDF to match Xarray\n",
    "synth_df2 = format_df(synth_df2, df2)\n",
    "\n",
    "synth_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(df):\n",
    "\n",
    "    print(df.isnull().values.any())\n",
    "    if df.isnull().values.any():\n",
    "        for idx, r in synth_df2.iterrows():\n",
    "            nulls = list(r[r.isnull()].index)\n",
    "            if nulls:\n",
    "                print(f'row {idx}: those columns are null:', nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(df2)\n",
    "\n",
    "check_missing(synth_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.height.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth Data\n",
    "#lats, lons, alts = get_dims(synth_df)\n",
    "#synth_df = glob_smooth(synth_df, lats, lons, alts)\n",
    "\n",
    "#Smooth Data\n",
    "lats, lons, alts = get_dims(synth_df2)\n",
    "synth_df2 = glob_smooth(synth_df2, lats, lons, alts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(synth_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Georgiy's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 51\n",
    "\n",
    "print( synth_df['height'][index]/9.81)\n",
    "print( synth_df['pressure'][index])\n",
    "plot_layer(synth_df, 'xy', synth_df['height'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Tristan's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altitude = 26250\n",
    "#synth_df2['longitude'] = synth_df2['longitude'].values[::-1]\n",
    "#print(synth_df2['longitude'])\n",
    "#print(synth_df2['latitude'][::-1])\n",
    "plot_layer(synth_df2, 'xy', altitude*9.81, skip =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_original(df2, synth_df2['height'][20]/9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_level_original(df,  synth_df['height'][20]/9.81)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Simulated NETCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToNETCDF(synth_df):\n",
    "\n",
    "    #reverse longitude\n",
    "    #synth_df['longitude'] = synth_df['longitude'].values[::-1]\n",
    "\n",
    "    alt = synth_df['height'].values#[::-1]\n",
    "    u_wind = synth_df['u_wind'].values\n",
    "    v_wind = synth_df['v_wind'].values\n",
    "    lat = synth_df['latitude'].values\n",
    "    lon = synth_df['longitude'].values#[::-1] #for Some reason this needs to be reversed for Xarray?\n",
    "    level = synth_df['pressure'].values#[::-1]\n",
    "    time = synth_df['timestep'].values\n",
    "    \n",
    "    dims = [len(np.unique(lat)), len(np.unique(lon)), len(np.unique(level)), len(np.unique(time))]\n",
    "    \n",
    "    lat_dim = len(np.unique(lat))\n",
    "    lon_dim = len(np.unique(lon))\n",
    "    level_dim = len(np.unique(level))\n",
    "    time_dim = len(np.unique(time))\n",
    "    \n",
    "    print(\"dims\", lat_dim, lon_dim, level_dim, time_dim)\n",
    "    \n",
    "    alt_split = alt.reshape((time_dim, lat_dim, lon_dim, level_dim))\n",
    "    u_wind_split = u_wind.reshape((time_dim, lat_dim, lon_dim, level_dim))\n",
    "    v_wind_split = v_wind.reshape((time_dim, lat_dim, lon_dim, level_dim))\n",
    "    \n",
    "    print(alt_split.shape)\n",
    "    print(np.transpose(alt_split, (0, 3, 1, 2)).shape)\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        data_vars = {\n",
    "            'z' : (['time','level','latitude','longitude'], np.transpose(alt_split,(0, 3, 1, 2))),\n",
    "            'u' : (['time','level','latitude','longitude'], np.transpose(u_wind_split, (0, 3, 1, 2))),\n",
    "            'v' : (['time','level','latitude','longitude'], np.transpose(v_wind_split, (0, 3, 1, 2))),\n",
    "        },\n",
    "    \n",
    "        coords={\n",
    "            'time' : np.unique(time),\n",
    "            'level' : np.unique(level),\n",
    "            'latitude' : np.unique(lat)[::-1],\n",
    "            'longitude' : np.unique(lon),\n",
    "            \n",
    "        },\n",
    "    \n",
    "        attrs={\n",
    "            'Conventions': 'CF-1.6'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #Reverse Longitude to match ERA5\n",
    "    ds = ds.reindex(longitude=ds.longitude[::-1])\n",
    "    ds['longitude'] = ds['longitude'].reindex(longitude=list(reversed(ds.longitude)))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = convertToNETCDF(synth_df2)\n",
    "ds.to_netcdf(r'/home/schuler/FLOW2D/forecasts/SynthCast.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs in each variable\n",
    "nan_check = ds.map(lambda x: np.isnan(x).any())\n",
    "\n",
    "print(nan_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Synth Winds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = None\n",
    "ds = xr.open_dataset(r'/home/schuler/FLOW2D/forecasts/SynthCast.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from era5.forecast import Forecast, Forecast_Subset\n",
    "from era5.forecast_visualizer import ForecastVisualizer\n",
    "\n",
    "filename = \"../../forecasts/SynthCast.nc\"\n",
    "FORECAST_PRIMARY = Forecast(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_subset = Forecast_Subset(FORECAST_PRIMARY)\n",
    "forecast_subset.assign_coord(0.5*(forecast_subset.Forecast.LAT_MAX + forecast_subset.Forecast.LAT_MIN),\n",
    "                              0.5*(forecast_subset.Forecast.LON_MAX + forecast_subset.Forecast.LON_MIN),\n",
    "                                \"2023-08-03T12:00:00.000000000\")\n",
    "forecast_subset.subset_forecast()\n",
    "\n",
    "#forecast_subset.ds = forecast_subset.ds.isel(level = slice(10,11))\n",
    "\n",
    "#Heights for Comparing\n",
    "print(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "\n",
    "avg_alt = np.average(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "print(avg_alt)\n",
    "#Round to nearest 250\n",
    "avg_alt = int(avg_alt/ 250) * 250\n",
    "print(avg_alt)\n",
    "\n",
    "forecast_subset.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_visualizer = ForecastVisualizer(forecast_subset)\n",
    "forecast_visualizer.generate_flow_array(timestamp = forecast_subset.ds.time.values[0])\n",
    "\n",
    "# Initialize Figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "#ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1 = fig.add_subplot(111, projection='custom3dquiver')\n",
    "# Manually add a CustomAxes3D to the figure\n",
    "#ax1 = Custom3DQuiver(fig)\n",
    "fig.add_axes(ax1)\n",
    "\n",
    "skip = 1\n",
    "\n",
    "#print(\"Saving Figure \" + str(timestamp))\n",
    "forecast_visualizer.visualize_3d_planar_flow(ax1, skip)\n",
    "#plt.savefig(str(i) +'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERA5 Forecast Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from era5.forecast import Forecast, Forecast_Subset\n",
    "from era5.forecast_visualizer import ForecastVisualizer\n",
    "\n",
    "filename = \"../../forecasts/Jan-2023-SEA.nc\"\n",
    "FORECAST_PRIMARY = Forecast(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_subset = Forecast_Subset(FORECAST_PRIMARY)\n",
    "forecast_subset.assign_coord(0.5*(forecast_subset.Forecast.LAT_MAX + forecast_subset.Forecast.LAT_MIN),\n",
    "                              0.5*(forecast_subset.Forecast.LON_MAX + forecast_subset.Forecast.LON_MIN),\n",
    "                                \"2023-01-01T00:00:00.000000000\")\n",
    "forecast_subset.subset_forecast()\n",
    "\n",
    "forecast_subset.ds = forecast_subset.ds.isel(level = slice(1,2))\n",
    "\n",
    "#Heights for Comparing\n",
    "print(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "\n",
    "avg_alt = np.average(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "print(avg_alt)\n",
    "#Round to nearest 250\n",
    "avg_alt = int(avg_alt/ 250) * 250\n",
    "print(avg_alt)\n",
    "\n",
    "forecast_subset.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_visualizer = ForecastVisualizer(forecast_subset)\n",
    "forecast_visualizer.generate_flow_array(timestamp = forecast_subset.ds.time.values[0])\n",
    "\n",
    "# Initialize Figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "#ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1 = fig.add_subplot(111, projection='custom3dquiver')\n",
    "# Manually add a CustomAxes3D to the figure\n",
    "#ax1 = Custom3DQuiver(fig)\n",
    "fig.add_axes(ax1)\n",
    "\n",
    "skip = 5\n",
    "\n",
    "#print(\"Saving Figure \" + str(timestamp))\n",
    "forecast_visualizer.visualize_3d_planar_flow(ax1, skip)\n",
    "#plt.savefig(str(i) +'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "alts = []\n",
    "\n",
    "slices = 6\n",
    "\n",
    "for i in range (slices):\n",
    "    #Reset Forecast Subset \n",
    "\n",
    "    forecast_subset.assign_coord(0.5*(forecast_subset.Forecast.LAT_MAX + forecast_subset.Forecast.LAT_MIN),\n",
    "                              0.5*(forecast_subset.Forecast.LON_MAX + forecast_subset.Forecast.LON_MIN),\n",
    "                                \"2023-01-01T00:00:00.000000000\");\n",
    "    forecast_subset.subset_forecast();\n",
    "    #Choose new slice\n",
    "    forecast_subset.ds = forecast_subset.ds.isel(level = slice(i,i+1))\n",
    "\n",
    "    #Regenerate Forecast Visualizer\n",
    "    forecast_visualizer = ForecastVisualizer(forecast_subset);\n",
    "    forecast_visualizer.generate_flow_array(timestamp = forecast_subset.ds.time.values[0]);\n",
    "\n",
    "\n",
    "    #Get alt info for comparison\n",
    "    avg_alt = np.average(forecast_subset.ds.z.values[0,0,0,:]/9.81)\n",
    "    #Round to nearest 250\n",
    "    avg_alt = int(avg_alt/ 250) * 250\n",
    "    #Append\n",
    "    alts.append(avg_alt)\n",
    "\n",
    "\n",
    "    # Initialize Figure\n",
    "    fig = plt.figure(figsize=(10, 8));\n",
    "    #ax1 = fig.add_subplot(111, projection='3d')\n",
    "    ax1 = fig.add_subplot(111, projection='custom3dquiver');\n",
    "    # Manually add a CustomAxes3D to the figure\n",
    "    #ax1 = Custom3DQuiver(fig)\n",
    "    fig.add_axes(ax1);\n",
    "    \n",
    "    skip = 5\n",
    "    \n",
    "    #print(\"Saving Figure \" + str(timestamp))\n",
    "    forecast_visualizer.visualize_3d_planar_flow(ax1, skip);\n",
    "\n",
    "    print(\"Saving Figure Level \" + str(forecast_subset.ds.isel(level = 0)))\n",
    "    plt.savefig(str(i) +'.png');\n",
    "\n",
    "with imageio.get_writer('ERA5-Slices.gif', mode='I', duration=1000, loop=0) as writer:\n",
    "    for i in range(slices):\n",
    "        image = imageio.imread(str(i) +'.png')\n",
    "        print(str(i) +'.png')\n",
    "        writer.append_data(image)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alts)\n",
    "# Path to your GIF file\n",
    "gif_path = 'ERA5-Slices.gif'\n",
    "# Display the GIF\n",
    "Image(filename=gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i in range(0,len(alts)):\n",
    "    altitude = alts[i]\n",
    "    plot_layer(synth_df2, 'xy', altitude*9.81);\n",
    "\n",
    "    plt.savefig(\"Radiosonde-\" + str(i) +'.png')\n",
    "\n",
    "with imageio.get_writer('Radiosonde-Slices.gif', mode='I', duration=1000, loop=0) as writer:\n",
    "    for i in range(slices):\n",
    "        image = imageio.imread(\"Radiosonde-\" + str(i) +'.png')\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your GIF file\n",
    "gif_path = 'Radiosonde-Slices.gif'\n",
    "# Display the GIF\n",
    "Image(filename=gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bigger Synthetic Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATE: 2023 10 1 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 1 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 2 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 2 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 3 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 3 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 4 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 4 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 5 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 5 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 6 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 6 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 7 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 7 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 8 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 8 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 9 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 9 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 10 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 10 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 11 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 11 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 12 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 12 12\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 13 0\n",
      "AGGREGATION... \n",
      "AGGREGATION : DONE.\n",
      "SYNTHESIZING...\n",
      "SYNTHESIS : DONE.\n",
      "False\n",
      "False\n",
      "SMOOTHING...\n",
      "SMOOTHING : DONE.\n",
      "False\n",
      "dims 61 101 46 1\n",
      "(1, 61, 101, 46)\n",
      "(1, 46, 61, 101)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n",
      "\n",
      "DATE: 2023 10 13 12\n",
      "AGGREGATION... \n"
     ]
    }
   ],
   "source": [
    "year = 2023\n",
    "month = 10\n",
    "day =1\n",
    "\n",
    "ds_list = []\n",
    "\n",
    "for d in range(day, 31+1):\n",
    "    for h in [0,12]:\n",
    "\n",
    "        print()\n",
    "        print(\"DATE:\", year,month,d,h)\n",
    "\n",
    "        # Aggregate Data\n",
    "        base_dir = r\"/mnt/d/RadioWinds/SOUNDINGS_DATA\" \n",
    "        df2 = aggregate_data2(base_dir, year, month, d, h)\n",
    "\n",
    "        #Match the Lat/Lon Ranges of the ERA5 subset\n",
    "        lat_min, lat_max = 30,    45       # US:  30,    45      SEA:  -12,   18\n",
    "        lon_min, lon_max = -125, -100    # US: -125, -100      SEA:   98,  131\n",
    "        alt_min, alt_max = 15000, 26250\n",
    "        #New Pressure levels for ERA5 download is 10-200hpa\n",
    "        \n",
    "        \n",
    "        #SEA:\n",
    "        #df2 = df2[(df2['longitude'] >= 90) & (df2['longitude'] <= 132) & (df2['latitude'] <= 18) &(df2['latitude'] >= -12.4)]\n",
    "\n",
    "        #Smaller SEA region:\n",
    "        \n",
    "        \n",
    "        #US SOuthwest:\n",
    "        df2 = df2[(df2['longitude'] >= lon_min) & (df2['longitude'] <= lon_max) & (df2['latitude'] >= lat_min) & (df2['latitude'] <= lat_max) ]\n",
    "        \n",
    "        df2 = df2[(df2['height'] >= alt_min) & (df2['height'] <= alt_max)] #Select desired altitude ranges\n",
    "        \n",
    "        step = .25\n",
    "        synth_df2 = create_synthetic_stations2(df2, lat_min, lat_max, lon_min, lon_max, step)\n",
    "                \n",
    "        #Format the SynthDF to match Xarray\n",
    "        synth_df2 = format_df(synth_df2, df2)\n",
    "\n",
    "        check_missing(df2)\n",
    "        check_missing(synth_df2)\n",
    "        \n",
    "        #Smooth Data\n",
    "        lats, lons, alts = get_dims(synth_df2)\n",
    "        synth_df2 = glob_smooth(synth_df2, lats, lons, alts)\n",
    "\n",
    "        check_missing(synth_df2)\n",
    "    \n",
    "        #Convert to Xarray\n",
    "        ds = convertToNETCDF(synth_df2)\n",
    "\n",
    "        # Check for NaNs in each variable\n",
    "        nan_check = ds.map(lambda x: np.isnan(x).any())\n",
    "        \n",
    "        print(nan_check)\n",
    "    \n",
    "        #Add to ds list\n",
    "        ds_list.append(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621 121.11825698085444\n",
      "21.06688084691621\n",
      "121.11825698085444\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (longitude: 101, time: 1, level: 46, latitude: 61)\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float64 -125.0 -124.8 -124.5 ... -100.5 -100.2 -100.0\n",
      "  * time       (time) datetime64[ns] 2023-10-12T12:00:00\n",
      "  * level      (level) float64 21.07 21.88 22.73 23.62 ... 112.0 116.5 121.1\n",
      "  * latitude   (latitude) object 45.0 44.75 44.5 44.25 ... 30.75 30.5 30.25 30.0\n",
      "Data variables:\n",
      "    z          (time, level, latitude, longitude) float64 2.575e+05 ... 1.472...\n",
      "    u          (time, level, latitude, longitude) float64 0.2466 ... 24.48\n",
      "    v          (time, level, latitude, longitude) float64 -2.889 ... 18.37\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "21.06688084691621 121.11825698085444 46\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "Frozen({'longitude': 101, 'time': 1, 'level': 46, 'latitude': 61})\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schuler/anaconda3/envs/FLOW2D/lib/python3.11/site-packages/xarray/core/concat.py:500: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Determine the global min and max levels\n",
    "all_levels = [ds.level.values for ds in ds_list]\n",
    "min_level = max(l.min() for l in all_levels)\n",
    "max_level = min(l.max() for l in all_levels)\n",
    "\n",
    "\n",
    "for l in all_levels:\n",
    "    print(l.min(), l.max())\n",
    "\n",
    "\n",
    "#print(all_levels)\n",
    "print(min_level)\n",
    "print(max_level)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ds_list_formatted = []\n",
    "\n",
    "print(ds_list[23])\n",
    "\n",
    "for d in ds_list_formatted:\n",
    "    print(d.dims)\n",
    "\n",
    "print()\n",
    "\n",
    "for ds in ds_list:\n",
    "    ds_list_formatted.append(ds.sel(level=slice(min_level, max_level)))\n",
    "\n",
    "\n",
    "all_levels_formatted = [ds.level.values for ds in ds_list_formatted]\n",
    "for l in all_levels_formatted:\n",
    "    print(l.min(), l.max(), len(l))\n",
    "\n",
    "#print(ds_list_formatted[-13])\n",
    "\n",
    "#del ds_list_formatted[-13]\n",
    "\n",
    "#sdfsdf\n",
    "\n",
    "# Concatenate along the 'level' dimension\n",
    "synthetic_forecast = xr.concat(ds_list_formatted, dim='time')\n",
    "\n",
    "#print(synthetic_forecast)\n",
    "\n",
    "\n",
    "for d in ds_list_formatted:\n",
    "    print(d.dims)\n",
    "    #print(d.z[0,:,0,0])\n",
    "\n",
    "\n",
    "# Check for NaNs in each variable\n",
    "nan_check = synthetic_forecast.map(lambda x: np.isnan(x).any())\n",
    "\n",
    "print(nan_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (longitude: 101, time: 62, level: 46, latitude: 61)\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float64 -125.0 -124.8 -124.5 ... -100.5 -100.2 -100.0\n",
       "  * time       (time) datetime64[ns] 2023-10-01 ... 2023-10-31T12:00:00\n",
       "  * level      (level) float64 21.07 21.88 22.73 23.62 ... 112.0 116.5 121.1\n",
       "  * latitude   (latitude) object 45.0 44.75 44.5 44.25 ... 30.75 30.5 30.25 30.0\n",
       "Data variables:\n",
       "    z          (time, level, latitude, longitude) float64 2.575e+05 ... 1.472...\n",
       "    u          (time, level, latitude, longitude) float64 1.821 1.819 ... 41.23\n",
       "    v          (time, level, latitude, longitude) float64 -2.64 -2.635 ... 10.31\n",
       "Attributes:\n",
       "    Conventions:  CF-1.6</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-672c8d86-fc91-42f8-b589-ae60e54d2586' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-672c8d86-fc91-42f8-b589-ae60e54d2586' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>longitude</span>: 101</li><li><span class='xr-has-index'>time</span>: 62</li><li><span class='xr-has-index'>level</span>: 46</li><li><span class='xr-has-index'>latitude</span>: 61</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-b5dbc2b6-a530-469e-9700-aac2e7a01bfe' class='xr-section-summary-in' type='checkbox'  checked><label for='section-b5dbc2b6-a530-469e-9700-aac2e7a01bfe' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-125.0 -124.8 ... -100.2 -100.0</div><input id='attrs-553c28da-a849-4102-a5a2-c68a2d435846' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-553c28da-a849-4102-a5a2-c68a2d435846' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8d95d1b6-16cc-4d0f-a724-394eae60e041' class='xr-var-data-in' type='checkbox'><label for='data-8d95d1b6-16cc-4d0f-a724-394eae60e041' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([-125.  , -124.75, -124.5 , -124.25, -124.  , -123.75, -123.5 , -123.25,\n",
       "       -123.  , -122.75, -122.5 , -122.25, -122.  , -121.75, -121.5 , -121.25,\n",
       "       -121.  , -120.75, -120.5 , -120.25, -120.  , -119.75, -119.5 , -119.25,\n",
       "       -119.  , -118.75, -118.5 , -118.25, -118.  , -117.75, -117.5 , -117.25,\n",
       "       -117.  , -116.75, -116.5 , -116.25, -116.  , -115.75, -115.5 , -115.25,\n",
       "       -115.  , -114.75, -114.5 , -114.25, -114.  , -113.75, -113.5 , -113.25,\n",
       "       -113.  , -112.75, -112.5 , -112.25, -112.  , -111.75, -111.5 , -111.25,\n",
       "       -111.  , -110.75, -110.5 , -110.25, -110.  , -109.75, -109.5 , -109.25,\n",
       "       -109.  , -108.75, -108.5 , -108.25, -108.  , -107.75, -107.5 , -107.25,\n",
       "       -107.  , -106.75, -106.5 , -106.25, -106.  , -105.75, -105.5 , -105.25,\n",
       "       -105.  , -104.75, -104.5 , -104.25, -104.  , -103.75, -103.5 , -103.25,\n",
       "       -103.  , -102.75, -102.5 , -102.25, -102.  , -101.75, -101.5 , -101.25,\n",
       "       -101.  , -100.75, -100.5 , -100.25, -100.  ])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2023-10-01 ... 2023-10-31T12:00:00</div><input id='attrs-21d5d66c-d829-40d3-a100-bd38466df0db' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-21d5d66c-d829-40d3-a100-bd38466df0db' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7b51fa7d-bd62-4c6c-94ee-6f83548a4ee0' class='xr-var-data-in' type='checkbox'><label for='data-7b51fa7d-bd62-4c6c-94ee-6f83548a4ee0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;2023-10-01T00:00:00.000000000&#x27;, &#x27;2023-10-01T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-02T00:00:00.000000000&#x27;, &#x27;2023-10-02T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-03T00:00:00.000000000&#x27;, &#x27;2023-10-03T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-04T00:00:00.000000000&#x27;, &#x27;2023-10-04T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-05T00:00:00.000000000&#x27;, &#x27;2023-10-05T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-06T00:00:00.000000000&#x27;, &#x27;2023-10-06T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-07T00:00:00.000000000&#x27;, &#x27;2023-10-07T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-08T00:00:00.000000000&#x27;, &#x27;2023-10-08T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-09T00:00:00.000000000&#x27;, &#x27;2023-10-09T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-10T00:00:00.000000000&#x27;, &#x27;2023-10-10T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-11T00:00:00.000000000&#x27;, &#x27;2023-10-11T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-12T00:00:00.000000000&#x27;, &#x27;2023-10-12T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-13T00:00:00.000000000&#x27;, &#x27;2023-10-13T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-14T00:00:00.000000000&#x27;, &#x27;2023-10-14T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-15T00:00:00.000000000&#x27;, &#x27;2023-10-15T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-16T00:00:00.000000000&#x27;, &#x27;2023-10-16T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-17T00:00:00.000000000&#x27;, &#x27;2023-10-17T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-18T00:00:00.000000000&#x27;, &#x27;2023-10-18T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-19T00:00:00.000000000&#x27;, &#x27;2023-10-19T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-20T00:00:00.000000000&#x27;, &#x27;2023-10-20T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-21T00:00:00.000000000&#x27;, &#x27;2023-10-21T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-22T00:00:00.000000000&#x27;, &#x27;2023-10-22T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-23T00:00:00.000000000&#x27;, &#x27;2023-10-23T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-24T00:00:00.000000000&#x27;, &#x27;2023-10-24T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-25T00:00:00.000000000&#x27;, &#x27;2023-10-25T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-26T00:00:00.000000000&#x27;, &#x27;2023-10-26T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-27T00:00:00.000000000&#x27;, &#x27;2023-10-27T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-28T00:00:00.000000000&#x27;, &#x27;2023-10-28T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-29T00:00:00.000000000&#x27;, &#x27;2023-10-29T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-30T00:00:00.000000000&#x27;, &#x27;2023-10-30T12:00:00.000000000&#x27;,\n",
       "       &#x27;2023-10-31T00:00:00.000000000&#x27;, &#x27;2023-10-31T12:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>level</span></div><div class='xr-var-dims'>(level)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>21.07 21.88 22.73 ... 116.5 121.1</div><input id='attrs-e864f2d7-6d5e-44f4-bd25-fda37e9d93bd' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-e864f2d7-6d5e-44f4-bd25-fda37e9d93bd' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3ec2f24c-1b08-4833-afb0-5ab6e7785991' class='xr-var-data-in' type='checkbox'><label for='data-3ec2f24c-1b08-4833-afb0-5ab6e7785991' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 21.066881,  21.883775,  22.733377,  23.617037,  24.536166,  25.49223 ,\n",
       "        26.486761,  27.521356,  28.59768 ,  29.717468,  30.882532,  32.094759,\n",
       "        33.356119,  34.668667,  36.034545,  37.455989,  38.935329,  40.475   ,\n",
       "        42.077539,  43.745594,  45.481927,  47.289423,  49.171089,  51.130065,\n",
       "        53.169629,  55.293119,  57.502315,  59.799962,  62.189608,  64.674944,\n",
       "        67.259811,  69.948203,  72.744273,  75.652345,  78.676914,  81.822655,\n",
       "        85.094435,  88.497314,  92.036554,  95.717633,  99.546246, 103.528318,\n",
       "       107.670013, 111.977743, 116.458179, 121.118257])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>45.0 44.75 44.5 ... 30.5 30.25 30.0</div><input id='attrs-b049bb86-5407-4bba-ac1f-8b698e3c0f35' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-b049bb86-5407-4bba-ac1f-8b698e3c0f35' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6168e92b-b9c3-4510-afb4-4a3ce108b26a' class='xr-var-data-in' type='checkbox'><label for='data-6168e92b-b9c3-4510-afb4-4a3ce108b26a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([45.0, 44.75, 44.5, 44.25, 44.0, 43.75, 43.5, 43.25, 43.0, 42.75, 42.5,\n",
       "       42.25, 42.0, 41.75, 41.5, 41.25, 41.0, 40.75, 40.5, 40.25, 40.0, 39.75,\n",
       "       39.5, 39.25, 39.0, 38.75, 38.5, 38.25, 38.0, 37.75, 37.5, 37.25, 37.0,\n",
       "       36.75, 36.5, 36.25, 36.0, 35.75, 35.5, 35.25, 35.0, 34.75, 34.5, 34.25,\n",
       "       34.0, 33.75, 33.5, 33.25, 33.0, 32.75, 32.5, 32.25, 32.0, 31.75, 31.5,\n",
       "       31.25, 31.0, 30.75, 30.5, 30.25, 30.0], dtype=object)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-a836066a-c05c-4ce3-94ff-be867caed12b' class='xr-section-summary-in' type='checkbox'  checked><label for='section-a836066a-c05c-4ce3-94ff-be867caed12b' class='xr-section-summary' >Data variables: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>z</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>2.575e+05 2.575e+05 ... 1.472e+05</div><input id='attrs-1f420915-322f-4642-b83a-afa78d7017ff' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1f420915-322f-4642-b83a-afa78d7017ff' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-44081e5b-cd7b-4dfa-a6eb-c60fb3cc42a6' class='xr-var-data-in' type='checkbox'><label for='data-44081e5b-cd7b-4dfa-a6eb-c60fb3cc42a6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[[[257512.5, 257512.5, 257512.5, ..., 257512.5, 257512.5,\n",
       "          257512.5],\n",
       "         [257512.5, 257512.5, 257512.5, ..., 257512.5, 257512.5,\n",
       "          257512.5],\n",
       "         [257512.5, 257512.5, 257512.5, ..., 257512.5, 257512.5,\n",
       "          257512.5],\n",
       "         ...,\n",
       "         [257512.5, 257512.5, 257512.5, ..., 257512.5, 257512.5,\n",
       "          257512.5],\n",
       "         [257512.5, 257512.5, 257512.5, ..., 257512.5, 257512.5,\n",
       "          257512.5],\n",
       "         [257512.5, 257512.5, 257512.5, ..., 257512.5, 257512.5,\n",
       "          257512.5]],\n",
       "\n",
       "        [[255060. , 255060. , 255060. , ..., 255060. , 255060. ,\n",
       "          255060. ],\n",
       "         [255060. , 255060. , 255060. , ..., 255060. , 255060. ,\n",
       "          255060. ],\n",
       "         [255060. , 255060. , 255060. , ..., 255060. , 255060. ,\n",
       "          255060. ],\n",
       "...\n",
       "         [149602.5, 149602.5, 149602.5, ..., 149602.5, 149602.5,\n",
       "          149602.5],\n",
       "         [149602.5, 149602.5, 149602.5, ..., 149602.5, 149602.5,\n",
       "          149602.5],\n",
       "         [149602.5, 149602.5, 149602.5, ..., 149602.5, 149602.5,\n",
       "          149602.5]],\n",
       "\n",
       "        [[147150. , 147150. , 147150. , ..., 147150. , 147150. ,\n",
       "          147150. ],\n",
       "         [147150. , 147150. , 147150. , ..., 147150. , 147150. ,\n",
       "          147150. ],\n",
       "         [147150. , 147150. , 147150. , ..., 147150. , 147150. ,\n",
       "          147150. ],\n",
       "         ...,\n",
       "         [147150. , 147150. , 147150. , ..., 147150. , 147150. ,\n",
       "          147150. ],\n",
       "         [147150. , 147150. , 147150. , ..., 147150. , 147150. ,\n",
       "          147150. ],\n",
       "         [147150. , 147150. , 147150. , ..., 147150. , 147150. ,\n",
       "          147150. ]]]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>u</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>1.821 1.819 1.814 ... 41.21 41.23</div><input id='attrs-128ef872-4497-4819-9d37-fababde1cd32' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-128ef872-4497-4819-9d37-fababde1cd32' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9c78a0e2-1e88-4102-9cd0-a019f59833fd' class='xr-var-data-in' type='checkbox'><label for='data-9c78a0e2-1e88-4102-9cd0-a019f59833fd' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[[[ 1.82146379e+00,  1.81910388e+00,  1.81444670e+00, ...,\n",
       "           6.07141582e+00,  6.07518743e+00,  6.07706230e+00],\n",
       "         [ 1.82633976e+00,  1.82396966e+00,  1.81929237e+00, ...,\n",
       "           6.06819353e+00,  6.07194556e+00,  6.07381071e+00],\n",
       "         [ 1.83604668e+00,  1.83365663e+00,  1.82893998e+00, ...,\n",
       "           6.06168058e+00,  6.06539361e+00,  6.06723938e+00],\n",
       "         ...,\n",
       "         [ 4.14691634e+00,  4.15135325e+00,  4.16017396e+00, ...,\n",
       "          -1.93812575e+00, -1.94124292e+00, -1.94277801e+00],\n",
       "         [ 4.15460005e+00,  4.15903636e+00,  4.16785502e+00, ...,\n",
       "          -1.98152787e+00, -1.98468560e+00, -1.98624099e+00],\n",
       "         [ 4.15846438e+00,  4.16290025e+00,  4.17171762e+00, ...,\n",
       "          -2.00332191e+00, -2.00650008e+00, -2.00806570e+00]],\n",
       "\n",
       "        [[ 2.07295145e+00,  2.07107234e+00,  2.06732051e+00, ...,\n",
       "           3.21152095e+00,  3.21001166e+00,  3.20922491e+00],\n",
       "         [ 2.07402268e+00,  2.07216831e+00,  2.06846594e+00, ...,\n",
       "           3.20921140e+00,  3.20763389e+00,  3.20681305e+00],\n",
       "         [ 2.07614746e+00,  2.07434224e+00,  2.07073810e+00, ...,\n",
       "           3.20462837e+00,  3.20291519e+00,  3.20202655e+00],\n",
       "...\n",
       "         [ 1.54642174e+01,  1.54861615e+01,  1.55297500e+01, ...,\n",
       "           4.51328116e+01,  4.51951067e+01,  4.52263145e+01],\n",
       "         [ 1.55021268e+01,  1.55240923e+01,  1.55677213e+01, ...,\n",
       "           4.51951931e+01,  4.52571865e+01,  4.52882419e+01],\n",
       "         [ 1.55212180e+01,  1.55431938e+01,  1.55868424e+01, ...,\n",
       "           4.52263395e+01,  4.52881797e+01,  4.53191577e+01]],\n",
       "\n",
       "        [[ 1.13381245e+01,  1.13568454e+01,  1.13942547e+01, ...,\n",
       "           3.04760048e+01,  3.05456422e+01,  3.05807405e+01],\n",
       "         [ 1.13482758e+01,  1.13669834e+01,  1.14043661e+01, ...,\n",
       "           3.04971920e+01,  3.05668924e+01,  3.06020220e+01],\n",
       "         [ 1.13684534e+01,  1.13871344e+01,  1.14244637e+01, ...,\n",
       "           3.05393487e+01,  3.06091731e+01,  3.06443641e+01],\n",
       "         ...,\n",
       "         [ 1.59849749e+01,  1.59983297e+01,  1.60250231e+01, ...,\n",
       "           4.10907360e+01,  4.11396800e+01,  4.11641095e+01],\n",
       "         [ 1.59995084e+01,  1.60128844e+01,  1.60396197e+01, ...,\n",
       "           4.11353819e+01,  4.11842223e+01,  4.12085989e+01],\n",
       "         [ 1.60067897e+01,  1.60201766e+01,  1.60469332e+01, ...,\n",
       "           4.11578285e+01,  4.12066168e+01,  4.12309667e+01]]]])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>v</span></div><div class='xr-var-dims'>(time, level, latitude, longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>-2.64 -2.635 -2.626 ... 10.28 10.31</div><input id='attrs-772987ac-baf2-4ce8-857f-6032c1687183' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-772987ac-baf2-4ce8-857f-6032c1687183' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d84db87c-93be-48fb-9569-4fa4f6b1a791' class='xr-var-data-in' type='checkbox'><label for='data-d84db87c-93be-48fb-9569-4fa4f6b1a791' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[[[-2.63994519e+00, -2.63533953e+00, -2.62608683e+00, ...,\n",
       "           3.60474772e+00,  3.60922919e+00,  3.61144240e+00],\n",
       "         [-2.64384101e+00, -2.63921898e+00, -2.62993370e+00, ...,\n",
       "           3.60081152e+00,  3.60528830e+00,  3.60749928e+00],\n",
       "         [-2.65158618e+00, -2.64693157e+00, -2.63758145e+00, ...,\n",
       "           3.59298573e+00,  3.59745325e+00,  3.59965986e+00],\n",
       "         ...,\n",
       "         [-1.89900296e+00, -1.88818312e+00, -1.86667270e+00, ...,\n",
       "          -6.27782391e-01, -6.33388752e-01, -6.36194773e-01],\n",
       "         [-1.88190875e+00, -1.87110377e+00, -1.84962449e+00, ...,\n",
       "          -6.59883150e-01, -6.65581947e-01, -6.68434393e-01],\n",
       "         [-1.87329758e+00, -1.86250021e+00, -1.84103685e+00, ...,\n",
       "          -6.76022109e-01, -6.81767376e-01, -6.84643160e-01]],\n",
       "\n",
       "        [[ 4.61673287e+00,  4.61613721e+00,  4.61495032e+00, ...,\n",
       "           3.81409159e+00,  3.81883508e+00,  3.82116559e+00],\n",
       "         [ 4.61775103e+00,  4.61718145e+00,  4.61604675e+00, ...,\n",
       "           3.81108233e+00,  3.81578885e+00,  3.81810097e+00],\n",
       "         [ 4.61976940e+00,  4.61925159e+00,  4.61822050e+00, ...,\n",
       "           3.80513074e+00,  3.80976368e+00,  3.81203924e+00],\n",
       "...\n",
       "         [-4.30317304e+00, -4.29274304e+00, -4.27203706e+00, ...,\n",
       "           6.94374068e+00,  6.97026097e+00,  6.98356712e+00],\n",
       "         [-4.28335965e+00, -4.27289294e+00, -4.25211433e+00, ...,\n",
       "           6.99291893e+00,  7.01938621e+00,  7.03266563e+00],\n",
       "         [-4.27338894e+00, -4.26290405e+00, -4.24208947e+00, ...,\n",
       "           7.01742163e+00,  7.04386093e+00,  7.05712624e+00]],\n",
       "\n",
       "        [[-5.37490790e+00, -5.38258882e+00, -5.39793345e+00, ...,\n",
       "          -4.07624617e+00, -3.97097961e+00, -3.91791315e+00],\n",
       "         [-5.38084256e+00, -5.38850170e+00, -5.40380270e+00, ...,\n",
       "          -4.04324984e+00, -3.93790698e+00, -3.88480299e+00],\n",
       "         [-5.39261534e+00, -5.40023094e+00, -5.41544477e+00, ...,\n",
       "          -3.97759192e+00, -3.87210020e+00, -3.81892304e+00],\n",
       "         ...,\n",
       "         [-3.34973367e+00, -3.35484029e+00, -3.36507374e+00, ...,\n",
       "           1.01722360e+01,  1.02175439e+01,  1.02401525e+01],\n",
       "         [-3.33658465e+00, -3.34175914e+00, -3.35212719e+00, ...,\n",
       "           1.02163094e+01,  1.02614844e+01,  1.02840256e+01],\n",
       "         [-3.33001954e+00, -3.33522819e+00, -3.34566398e+00, ...,\n",
       "           1.02384569e+01,  1.02835656e+01,  1.03060731e+01]]]])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-64c597be-3ca6-4987-9bee-ebab67ec46b9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-64c597be-3ca6-4987-9bee-ebab67ec46b9' class='xr-section-summary' >Attributes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>Conventions :</span></dt><dd>CF-1.6</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (longitude: 101, time: 62, level: 46, latitude: 61)\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float64 -125.0 -124.8 -124.5 ... -100.5 -100.2 -100.0\n",
       "  * time       (time) datetime64[ns] 2023-10-01 ... 2023-10-31T12:00:00\n",
       "  * level      (level) float64 21.07 21.88 22.73 23.62 ... 112.0 116.5 121.1\n",
       "  * latitude   (latitude) object 45.0 44.75 44.5 44.25 ... 30.75 30.5 30.25 30.0\n",
       "Data variables:\n",
       "    z          (time, level, latitude, longitude) float64 2.575e+05 ... 1.472...\n",
       "    u          (time, level, latitude, longitude) float64 1.821 1.819 ... 41.23\n",
       "    v          (time, level, latitude, longitude) float64 -2.64 -2.635 ... 10.31\n",
       "Attributes:\n",
       "    Conventions:  CF-1.6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export to Netcdf\n",
    "synthetic_forecast.to_netcdf(r'/mnt/d/FORECASTS/SYNTH-Oct-2023-USA.nc')\n",
    "synthetic_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        bool False\n",
      "    u        bool False\n",
      "    v        bool False\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in each variable\n",
    "nan_check = synthetic_forecast.map(lambda x: np.isnan(x).any())\n",
    "\n",
    "print(nan_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( synthetic_forecast.z[5,:,0,0]/9.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
